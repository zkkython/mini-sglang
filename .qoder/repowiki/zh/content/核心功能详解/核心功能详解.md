# 核心功能详解

<cite>
**本文档引用的文件**
- [radix_manager.py](file://python/minisgl/kvcache/radix_manager.py)
- [prefill.py](file://python/minisgl/scheduler/prefill.py)
- [graph.py](file://python/minisgl/engine/graph.py)
- [scheduler.py](file://python/minisgl/scheduler/scheduler.py)
- [cache.py](file://python/minisgl/scheduler/cache.py)
- [args.py](file://python/minisgl/server/args.py)
- [engine.py](file://python/minisgl/engine/engine.py)
- [features.md](file://docs/features.md)
</cite>

## 目录
1. [Radix Cache](#radix-cache)
2. [Chunked Prefill](#chunked-prefill)
3. [Overlap Scheduling](#overlap-scheduling)
4. [CUDA Graph](#cuda-graph)
5. [配置参数与调优建议](#配置参数与调优建议)
6. [性能对比与价值分析](#性能对比与价值分析)

## Radix Cache

Radix Cache 是一种高效的键值缓存（KV Cache）管理策略，旨在解决大模型推理中因重复计算导致的性能浪费问题。当多个请求共享相同的输入前缀（如系统提示词或对话历史）时，传统的缓存机制会为每个请求独立存储KV缓存，造成存储和计算资源的浪费。Radix Cache 通过构建一个基数树（Radix Tree）结构，将共享前缀的KV缓存进行合并存储，从而实现缓存的高效复用。

该技术的核心实现在 `python/minisgl/kvcache/radix_manager.py` 文件中。`RadixCacheManager` 类继承自 `BaseCacheManager`，利用 `RadixTreeNode` 构建树形结构来管理缓存。每个节点代表一个输入ID序列的片段，其 `value` 字段存储了该片段对应的KV缓存页索引。当新请求到达时，`match_prefix` 方法会遍历树结构，寻找最长匹配前缀，并返回已缓存的KV索引，从而避免对已计算部分的重新推理。`insert_prefix` 方法则负责将新请求的未缓存部分插入树中。为了防止正在使用的缓存被错误地驱逐，`lock_handle` 方法通过引用计数（`ref_count`）来保护活跃的缓存节点。

启用 Radix Cache 可以显著减少长上下文场景下的计算量，提高吞吐量。尤其是在处理大量具有相同系统提示的并发请求时，其性能优势尤为明显。根据 `docs/features.md` 文档，此功能默认启用，用户可以通过 `--cache` 参数切换为朴素的缓存管理策略。

**本节来源**
- [radix_manager.py](file://python/minisgl/kvcache/radix_manager.py#L1-L221)
- [cache.py](file://python/minisgl/scheduler/cache.py#L24-L27)
- [features.md](file://docs/features.md#L42-L47)

## Chunked Prefill

Chunked Prefill（分块预填充）技术旨在解决长提示（long prompt）在预填充（prefill）阶段导致的显存峰值过高和内存溢出（OOM）问题。在传统的推理流程中，整个输入提示会一次性送入模型进行处理，这要求显存能够容纳整个提示的KV缓存，对于长文本来说，这可能迅速耗尽显存。

Chunked Prefill 将长提示分割成多个较小的块（chunk），然后分批进行预填充处理。该机制在 `python/minisgl/scheduler/prefill.py` 文件中的 `PrefillAdder` 和 `PrefillManager` 类中实现。`PrefillAdder` 的 `_try_allocate_one` 方法在为请求分配资源时，会检查其扩展长度（`extend_len`）是否超过预填充预算（`token_budget`）。如果超过，`_add_one_req` 方法会创建一个 `ChunkedReq` 对象，该对象的 `is_chunked` 属性为 `True`，表示此请求需要分块处理。`PrefillManager` 的 `schedule_next_batch` 方法则负责调度这些分块请求，确保每次只处理一个块，从而将显存占用控制在一个可管理的水平。

此技术通过牺牲少量的计算效率（由于分块处理引入的额外调度开销），换取了处理超长上下文的能力和显存使用的稳定性。`docs/features.md` 文档明确指出，此功能默认启用，用户可以通过 `--max-prefill-length` 参数来配置最大块大小。

**本节来源**
- [prefill.py](file://python/minisgl/scheduler/prefill.py#L1-L154)
- [features.md](file://docs/features.md#L28-L30)

## Overlap Scheduling

Overlap Scheduling（重叠调度）是一种用于隐藏CPU调度开销的技术，旨在提高GPU的利用率和系统的整体吞吐量。在推理过程中，存在CPU和GPU交替工作的阶段：CPU负责调度、准备数据和管理请求，而GPU负责执行模型计算。如果这两个阶段是串行执行的，GPU在等待CPU准备数据时会处于空闲状态，造成资源浪费。

Overlap Scheduling 通过使用CUDA流（CUDA stream）实现了CPU和GPU工作的重叠。该机制在 `python/minisgl/scheduler/scheduler.py` 文件的 `Scheduler` 类中实现。`Scheduler` 维护了两个CUDA流：一个用于引擎（`engine.stream`），另一个用于调度器自身（`self.stream`）。在 `overlap_loop` 方法中，当GPU在引擎流上执行当前批次的推理时，CPU可以在调度器流上并行地处理上一个批次的输出结果（如解码token、发送响应、处理新请求等）。`self.engine.stream.wait_stream(self.stream)` 这行代码确保了GPU计算流会等待CPU调度流完成必要的准备工作，从而实现了无缝衔接。

通过这种重叠执行，CPU的调度开销被有效地“隐藏”在了GPU的计算时间之内，减少了GPU的空闲等待时间，显著提升了系统的吞吐量。`docs/features.md` 文档提到，此技术受 NanoFlow 启发，默认启用，可通过设置环境变量 `DISABLE_OVERLAP_SCHEDULING` 来禁用。

**本节来源**
- [scheduler.py](file://python/minisgl/scheduler/scheduler.py#L88-L280)
- [features.md](file://docs/features.md#L49-L54)

## CUDA Graph

CUDA Graph 技术用于优化解码（decode）阶段的性能，主要目标是减少GPU内核启动的CPU开销。在解码阶段，每个生成的token都需要执行一系列相同或相似的GPU内核操作。频繁地启动这些内核会产生显著的CPU开销，成为性能瓶颈。

CUDA Graph 通过“捕获”（capture）一个完整的计算流程（包括内核启动、内存拷贝等）并将其序列化为一个图（graph），然后在后续执行中通过一次“重放”（replay）调用来执行整个图，从而将多次内核启动的开销减少为一次。该功能在 `python/minisgl/engine/graph.py` 文件的 `GraphRunner` 类中实现。在引擎初始化时，`GraphRunner` 会为一系列预定义的批处理大小（`cuda_graph_bs`）捕获CUDA图。`_determine_cuda_graph_bs` 函数会根据可用显存自动确定要捕获的批处理大小列表。`can_use_cuda_graph` 方法检查当前批次是否适合使用CUDA图（必须是解码阶段且批处理大小不超过最大图批处理大小）。如果满足条件，`replay` 方法会调用已捕获的图来执行推理，否则回退到常规的逐内核启动模式。

启用CUDA Graph可以显著降低解码阶段的延迟，提高小批量请求的吞吐量。`docs/features.md` 文档指出，此功能默认启用，用户可以通过 `--cuda-graph-max-bs` 参数来控制最大捕获的批处理大小。

**本节来源**
- [graph.py](file://python/minisgl/engine/graph.py#L1-L156)
- [engine.py](file://python/minisgl/engine/engine.py#L196-L211)
- [features.md](file://docs/features.md#L38-L40)

## 配置参数与调优建议

mini-sglang 提供了丰富的命令行参数来配置和调优其四大核心技术。这些参数主要在 `python/minisgl/server/args.py` 文件中定义。

- **`--cache`**: 用于选择KV缓存管理策略。可选值为 `naive` 或 `radix`。默认为 `radix`。在需要处理大量共享前缀的请求时，保持默认值以获得最佳性能。在请求前缀差异性很大的场景下，两种策略的性能差异不大。
- **`--max-prefill-length`** (或 `--max-extend-length`): 设置Chunked Prefill的最大块大小。默认值通常由模型和系统决定。如果遇到长提示的OOM问题，可以尝试降低此值。但需注意，设置过小（如128）会因频繁的调度开销而严重降低性能。
- **`--cuda-graph-max-bs`** (或 `--graph`): 设置用于捕获CUDA图的最大批处理大小。默认值会根据GPU显存自动调整。增加此值可以覆盖更大的批处理场景，但会消耗更多显存用于存储图。如果显存紧张，可以适当降低此值或设为0来完全禁用CUDA Graph。
- **`--shell`**: 启用交互式shell模式，此模式会自动设置 `--cuda-graph-max-bs 1` 和 `--max-running-req 1`，禁用CUDA Graph和重叠调度，以简化调试。

**调优建议**:
1.  **默认配置优先**: 对于大多数场景，使用默认配置即可获得良好的性能。
2.  **长提示处理**: 当处理超长提示时，确保 `--max-prefill-length` 设置得当，以平衡显存使用和计算效率。
3.  **高并发场景**: 在高并发、请求前缀相似的场景下，Radix Cache和CUDA Graph是提升吞吐量的关键。
4.  **显存受限**: 如果显存不足，可以考虑降低 `--cuda-graph-max-bs` 或 `--max-prefill-length`，甚至禁用CUDA Graph。

**本节来源**
- [args.py](file://python/minisgl/server/args.py#L1-L235)
- [features.md](file://docs/features.md)

## 性能对比与价值分析

通过对比启用和禁用这些功能的性能差异，可以清晰地理解它们的价值。

- **Radix Cache**: 在处理具有相同系统提示的100个并发请求时，启用Radix Cache可以将预填充阶段的计算量减少90%以上，因为只有第一个请求需要完整计算，后续请求可以直接复用大部分KV缓存。这直接转化为更高的吞吐量和更低的延迟。
- **Chunked Prefill**: 对于一个8192长度的提示，启用Chunked Prefill可以将预填充阶段的峰值显存占用从一次性分配8192个token的KV缓存，降低到按块（如2048）分配，有效避免了OOM，并允许系统处理更长的上下文。
- **Overlap Scheduling**: 通过将CPU调度开销与GPU计算重叠，可以将GPU的利用率从60%提升至85%以上，显著减少了GPU的空闲时间，从而提高了整体吞吐量。
- **CUDA Graph**: 在解码阶段，启用CUDA Graph可以将每个token生成的CPU开销从数百微秒降低到几十微秒，对于小批量请求，端到端延迟可降低30%以上。

综上所述，这四大核心技术共同构成了mini-sglang高性能推理的基石。Radix Cache和Chunked Prefill解决了长上下文场景下的计算和显存瓶颈，而Overlap Scheduling和CUDA Graph则通过优化执行流程，最大限度地发挥了硬件的性能潜力。它们的协同工作使得mini-sglang能够在保证高吞吐量和低延迟的同时，高效地处理复杂的推理任务。

**本节来源**
- [features.md](file://docs/features.md)
- [bench.py](file://benchmark/offline/bench.py)
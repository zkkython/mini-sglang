# Mini-SGLang 请求生命周期分析

## 概述
本文档详细分析一个请求在mini-sGLang中的完整处理流程，从HTTP API接收到最终响应返回。

## 请求处理流程图
```
客户端 → HTTP API → FrontendManager → Tokenizer Worker → Scheduler → Engine → Detokenizer → 客户端
```

## 阶段一：API接收与前端处理

### 1. HTTP请求接收
**位置**: `api_server.py:245-277` (v1_completions函数)
**处理组件**: FastAPI + uvicorn
**输入**: OpenAI兼容的JSON请求
**关键步骤**:
```python
@app.post("/v1/chat/completions")
async def v1_completions(req: OpenAICompletionRequest):
    state = get_global_state()  # 获取FrontendManager实例
    uid = state.new_user()      # 分配用户ID
    await state.send_one(       # 发送TokenizeMsg
        TokenizeMsg(
            uid=uid,
            text=prompt,
            sampling_params=SamplingParams(...)
        )
    )
```

### 2. FrontendManager处理
**位置**: `api_server.py:100-196` (FrontendManager类)
**关键组件**:
- `new_user()`: 分配唯一UID，创建事件和确认映射
- `send_one()`: 发送消息到Tokenizer Worker
- `wait_for_ack()`: 等待并接收响应

**消息流**:
1. 创建`TokenizeMsg`包含用户输入和采样参数
2. 通过`ZmqAsyncPushQueue`发送到Tokenizer Worker
3. 启动监听任务接收响应

## 阶段二：分词处理

### 3. Tokenizer Worker处理
**位置**: `tokenizer/server.py:30-100` (tokenize_worker函数)
**关键步骤**:
```python
def tokenize_worker(...):
    # 初始化ZMQ队列
    send_backend = ZmqPushQueue(backend_addr, ...)  # 到Scheduler
    send_frontend = ZmqPushQueue(frontend_addr, ...) # 到Frontend
    recv_listener = ZmqPullQueue(addr, ...)          # 从Frontend接收

    # 消息处理循环
    while True:
        pending_msg = _unwrap_msg(recv_listener.get())

        # 分离TokenizeMsg和DetokenizeMsg
        tokenize_msg = [m for m in pending_msg if isinstance(m, TokenizeMsg)]

        if len(tokenize_msg) > 0:
            # 分词处理
            tensors = tokenize_manager.tokenize(tokenize_msg)

            # 创建UserMsg发送到后端
            batch_output = BatchBackendMsg(
                data=[
                    UserMsg(
                        uid=msg.uid,
                        input_ids=t,  # CPU tensor
                        sampling_params=msg.sampling_params,
                    )
                    for msg, t in zip(tokenize_msg, tensors)
                ]
            )
            send_backend.put(batch_output)
```

**消息转换**:
- `TokenizeMsg` → `UserMsg` (包含tokenized input_ids)

## 阶段三：调度与推理

### 4. Scheduler接收处理
**位置**: `scheduler/scheduler.py:118-141` (_process_one_msg函数)
**关键步骤**:
```python
def _process_one_msg(self, msg: BaseBackendMsg):
    elif isinstance(msg, UserMsg):
        # 验证序列长度
        input_len = len(msg.input_ids)
        max_seq_len = self.engine.max_seq_len

        # 调整输出长度限制
        if msg.sampling_params.max_tokens > max_output_len:
            msg.sampling_params.max_tokens = max_output_len

        # 添加到prefill管理器
        self.prefill_manager.add_one_req(msg)
```

### 5. Prefill阶段调度
**位置**: `scheduler/prefill.py` (PrefillManager)
**功能**:
- 收集新请求
- 批量处理（Chunked Prefill）
- 将请求转换为Batch对象

**关键数据结构**:
```python
@dataclass(eq=False)
class Req:
    input_ids: torch.Tensor    # CPU tensor
    table_idx: int             # 缓存表索引
    cached_len: int            # 已缓存长度
    output_len: int            # 输出长度
    uid: int                   # 用户ID
    sampling_params: SamplingParams
    cache_handle: BaseCacheHandle

@dataclass
class Batch:
    reqs: List[Req]            # 请求列表
    out_loc: torch.Tensor      # 输出位置
    positions: torch.Tensor    # 位置编码
```

### 6. 推理引擎执行
**位置**: `engine/engine.py` (Engine类)
**关键步骤**:
1. **Batch准备**: `_prepare_batch()`
   - 分配KV缓存空间
   - 准备位置编码
   - 设置注意力元数据

2. **前向传播**: `forward_batch()`
   - 执行模型推理
   - 使用FlashAttention或FlashInfer后端
   - 支持CUDA图优化

3. **采样**: `sampler.sample()`
   - 根据温度、top_k、top_p采样下一个token

4. **解码循环**: 重复生成直到达到max_tokens或遇到EOS

## 阶段四：结果返回

### 7. Detokenizer处理
**位置**: `tokenizer/server.py:68-82`
**处理流程**:
```python
if len(detokenize_msg) > 0:
    # 反分词处理
    replies = detokenize_manager.detokenize(detokenize_msg)

    # 创建UserReply返回前端
    batch_output = BatchFrontendMsg(
        data=[
            UserReply(
                uid=msg.uid,
                incremental_output=reply,  # 生成的文本
                finished=msg.finished,     # 是否完成
            )
            for msg, reply in zip(detokenize_msg, replies)
        ]
    )
    send_frontend.put(batch_output)
```

**消息转换**:
- `DetokenizeMsg` → `UserReply` (包含生成文本)

### 8. FrontendManager响应流式输出
**位置**: `api_server.py:151-187`
**流式响应**:
```python
async def stream_chat_completions(self, uid: int):
    first_chunk = True
    async for ack in self.wait_for_ack(uid):
        delta = {}
        if first_chunk:
            delta["role"] = "assistant"
            first_chunk = False
        if ack.incremental_output:
            delta["content"] = ack.incremental_output

        # 创建SSE格式响应
        chunk = {
            "id": f"cmpl-{uid}",
            "object": "text_completion.chunk",
            "choices": [{"delta": delta, "index": 0, "finish_reason": None}],
        }
        yield f"data: {json.dumps(chunk)}\n\n".encode()
```

## 关键消息类型

### 1. TokenizeMsg → 前端到分词器
```python
@dataclass
class TokenizeMsg(BaseTokenizerMsg):
    uid: int
    text: str | List[Dict[str, str]]  # 输入文本
    sampling_params: SamplingParams   # 采样参数
```

### 2. UserMsg → 分词器到调度器
```python
@dataclass
class UserMsg(BaseBackendMsg):
    uid: int
    input_ids: torch.Tensor        # CPU 1D int32 tensor
    sampling_params: SamplingParams
```

### 3. UserReply → 反分词器到前端
```python
@dataclass
class UserReply(BaseFrontendMsg):
    uid: int
    incremental_output: str        # 增量输出文本
    finished: bool                 # 是否完成
```

## 性能优化特性

### 1. Chunked Prefill
- 将长序列分成多个chunk处理
- 减少峰值内存使用
- 支持长上下文推理

### 2. Overlap Scheduling
- CPU调度与GPU计算重叠
- 隐藏调度开销
- 提高吞吐量

### 3. KV缓存管理
- Radix Cache: 共享前缀缓存
- Naive Cache: 简单分配策略
- 支持高效缓存重用

### 4. CUDA图优化
- 捕获计算图减少内核启动开销
- 支持动态形状
- 自动图选择策略

## 调试与监控

### 日志追踪点
1. **API接收**: `Received generate request`
2. **分词处理**: `Received {len(pending_msg)} messages`
3. **调度处理**: `Received user msg`
4. **推理执行**: `Forward batch size={batch_size}`
5. **流式输出**: `Finished streaming response`

### 性能指标
1. **端到端延迟**: 请求到响应时间
2. **Token生成速度**: tokens/秒
3. **GPU利用率**: 计算与内存使用
4. **批处理效率**: 实际vs理论吞吐量

## 常见问题与排查

### 1. 请求超时
**可能原因**:
- 模型加载慢
- GPU内存不足
- 序列过长

**解决方案**:
- 检查`--max-running-requests`设置
- 监控GPU内存使用
- 启用详细日志

### 2. 响应缓慢
**可能原因**:
- 批处理大小不足
- 调度策略低效
- 硬件瓶颈

**解决方案**:
- 调整`--max-extend-length`
- 优化调度参数
- 性能分析工具定位瓶颈

### 3. 内存溢出
**可能原因**:
- KV缓存比例过高
- 序列长度超限
- 并发请求过多

**解决方案**:
- 调整`--memory-ratio`
- 设置`--max-seq-len-override`
- 限制并发请求数

## 学习总结

### 已掌握流程
1. ✅ HTTP API接收与验证
2. ✅ 前端消息管理与流式响应
3. ✅ 分词与反分词处理
4. ✅ 调度器请求管理与批处理
5. ✅ 推理引擎执行流程
6. ✅ 结果返回与流式输出

### 关键技术点
1. **ZMQ进程间通信**: 高性能消息传递
2. **异步编程模型**: asyncio协调多组件
3. **流式响应**: Server-Sent Events
4. **批处理优化**: 动态批处理策略
5. **缓存管理**: 高效KV缓存利用

### 下一步研究方向
1. 深入分析调度算法
2. 研究KV缓存管理策略
3. 探索注意力后端实现
4. 性能调优与基准测试
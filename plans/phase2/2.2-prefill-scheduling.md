# 2.2 调度器 Prefill 阶段分析

> 源文件：`python/minisgl/scheduler/prefill.py`, `scheduler/cache.py`, `scheduler/table.py`

## 概览

Prefill 阶段负责处理新到达的请求，将用户的完整 prompt 送入模型计算 KV cache。这是推理的第一步，也是计算量最大的阶段。

核心组件：
```
PrefillManager  →  管理待处理请求队列，调度 prefill batch
PrefillAdder    →  资源分配器，决定哪些请求可以进入本轮 batch
ChunkedReq      →  支持分块 prefill 的特殊 Req 子类
CacheManager    →  KV缓存页面分配与前缀匹配
TableManager    →  page_table 槽位管理
```

---

## 1. PrefillManager - Prefill 调度管理器

```python
@dataclass
class PrefillManager:
    cache_manager: CacheManager
    table_manager: TableManager
    decode_manager: DecodeManager
    pending_list: List[PendingReq] = field(default_factory=list)
```

### 核心流程

```
add_one_req(UserMsg)          # 新请求入队
    │
    ▼
pending_list: [req1, req2, req3, ...]   # FIFO 队列
    │
    ▼
schedule_next_batch(budget)   # 调度器主循环调用
    │
    ▼
Batch(reqs=[...], phase="prefill")      # 返回 prefill batch
```

### `schedule_next_batch()` 详解

```python
def schedule_next_batch(self, prefill_budget: int) -> Batch | None:
    if len(self.pending_list) == 0:
        return None

    adder = PrefillAdder(
        token_budget=prefill_budget,           # 本轮最多处理的 token 数
        reserved_size=self.decode_manager.inflight_tokens,  # 为 decode 预留的空间
        cache_manager=self.cache_manager,
        table_manager=self.table_manager,
    )

    reqs, chunked_list = [], []
    for pending_req in self.pending_list:
        if req := adder.try_add_one(pending_req):
            pending_req.chunked_req = None
            if isinstance(req, ChunkedReq):    # 分块了，还没处理完
                pending_req.chunked_req = req
                chunked_list.append(pending_req)
            reqs.append(req)
        else:
            break                              # 资源不够，停止添加

    if len(reqs) == 0:
        return None
    # 分块请求优先 + 剩余未处理请求
    self.pending_list = chunked_list + self.pending_list[len(reqs):]
    return Batch(reqs=reqs, phase="prefill")
```

关键点：
- `token_budget` 限制单轮 prefill 的总 token 数（由 `config.max_extend_tokens` 决定）
- `reserved_size` 考虑了正在 decode 的请求所需的空间，避免 OOM
- 分块请求（ChunkedReq）会被放回队列头部，下轮优先处理

---

## 2. PrefillAdder - 资源分配器

PrefillAdder 是单次调度的核心决策者，负责判断每个请求是否有足够资源进入 batch。

### 资源检查：`_try_allocate_one()`

```
检查 table 槽位 → 前缀匹配 → 估算所需空间 → 锁定缓存 → 分配 table_idx
```

```python
def _try_allocate_one(self, req: PendingReq):
    # 1. 检查 table 是否有空闲槽位
    if self.table_manager.available_size == 0:
        return None

    # 2. 前缀匹配：复用已有的 KV cache
    handle, match_indices = self.cache_manager.match_req(req)
    cached_len = handle.cached_len

    # 3. 估算所需空间
    extend_len = req.input_len - cached_len
    estimated_len = extend_len + req.output_len

    # 4. 检查缓存空间是否足够
    if estimated_len + self.reserved_size > self.cache_manager.available_size:
        return None

    # 5. 锁定缓存句柄（防止被驱逐）
    self.cache_manager.lock(handle)

    # 6. 分配 table_idx
    table_idx = self.table_manager.allocate()
    # 7. 复制已缓存部分的 token ids 和 page 映射
    ...
    return handle, table_idx
```

### 请求添加：`_add_one_req()`

```python
def _add_one_req(self, pending_req, cache_handle, table_idx, cached_len) -> Req:
    remain_len = pending_req.input_len - cached_len
    chunk_size = min(self.token_budget, remain_len)
    is_chunked = chunk_size < remain_len       # 预算不够，需要分块

    CLS = ChunkedReq if is_chunked else Req
    self.token_budget -= chunk_size
    self.reserved_size += remain_len + pending_req.output_len

    # 将 token ids 复制到 GPU 上的 token_pool
    device_ids = self.table_manager.token_pool[table_idx][slice]
    device_ids.copy_(pending_req.input_ids[slice].pin_memory(), non_blocking=True)

    return CLS(input_ids=..., table_idx=table_idx, cached_len=cached_len, ...)
```

---

## 3. ChunkedReq - 分块 Prefill

当一个请求的 prompt 太长，超过了本轮的 `token_budget` 时，会创建 ChunkedReq 而非 Req。

```python
class ChunkedReq(Req):
    def append_host(self, next_token):
        raise NotImplementedError("ChunkedReq should not be sampled")

    @property
    def can_decode(self) -> bool:
        return False    # 不参与 decode，不会被采样
```

### 分块 Prefill 流程

```
请求 prompt = 2000 tokens, budget = 512

第1轮: ChunkedReq(cached_len=0, device_len=512)
       → 计算前512个token的KV cache
       → complete_one() 后 cached_len=512
       → 回到 pending_list 头部

第2轮: ChunkedReq(cached_len=512, device_len=1024)
       → 计算 token 512-1023 的KV cache
       → 回到 pending_list 头部

第3轮: ChunkedReq(cached_len=1024, device_len=1536)
       → 继续...

第4轮: Req(cached_len=1536, device_len=2000)
       → 最后一块，创建正常 Req
       → 进入 decode 阶段
```

---

## 4. CacheManager - 缓存管理

```python
class CacheManager:
    _free_slots: torch.Tensor    # 空闲页面索引
    manager: BaseCacheManager    # 底层缓存管理器（Naive/Radix）
```

### 核心操作

| 方法 | 作用 |
|------|------|
| `match_req(req)` | 前缀匹配，返回 (handle, matched_indices) |
| `allocate(n)` | 分配 n 个页面，不够时触发驱逐 |
| `lock(handle)` | 锁定缓存，防止被驱逐 |
| `free_and_cache_finished_req()` | 请求完成后，将 KV cache 插入缓存树供复用 |

### 分配与驱逐

```python
def allocate(self, needed_len):
    if needed_len <= len(self._free_slots):
        # 直接从空闲列表分配
        allocated = self._free_slots[:needed_len]
        self._free_slots = self._free_slots[needed_len:]
        return allocated

    # 空闲不够，触发驱逐
    evicted = self.manager.evict(needed_len - len(self._free_slots))
    merged = torch.cat([self._free_slots, evicted])
    allocated = merged[:needed_len]
    self._free_slots = merged[needed_len:]
    return allocated
```

---

## 5. TableManager - 槽位管理

```python
class TableManager:
    _free_slots: list[int]           # 空闲 table 行索引
    page_table: torch.Tensor         # [max_running_req+1, max_seq_len] GPU tensor
    token_pool: torch.Tensor         # 同形状，存储 token ids
```

TableManager 管理 `page_table` 的行分配。每个 Req 占用一行，行号就是 `table_idx`。

- `allocate()` → `_free_slots.pop()` 返回一个空闲行号
- `free(slot)` → `_free_slots.append(slot)` 归还行号

---

## 6. 完整 Prefill 调度流程图

```
Scheduler._schedule_next_batch()
    │
    ▼
PrefillManager.schedule_next_batch(budget=max_extend_tokens)
    │
    ├─ 创建 PrefillAdder(budget, reserved=decode_inflight)
    │
    ├─ 遍历 pending_list:
    │   │
    │   ▼ adder.try_add_one(pending_req)
    │   │
    │   ├─ 已有 chunked_req? → 直接续传
    │   │
    │   └─ 新请求:
    │       ├─ CacheManager.match_req() → 前缀匹配
    │       ├─ 估算空间 (extend + output + reserved)
    │       ├─ CacheManager.lock(handle)
    │       ├─ TableManager.allocate() → table_idx
    │       └─ 复制 cached 部分到 GPU
    │
    │   ├─ budget 够? → 创建 Req
    │   └─ budget 不够? → 创建 ChunkedReq, 放回队列头部
    │
    ▼
Batch(reqs=[req1, req2, ...], phase="prefill")
    │
    ▼
Scheduler._prepare_batch(batch)
    ├─ CacheManager.allocate(needed_size) → out_loc
    ├─ GraphRunner.pad_batch() → padded_reqs
    ├─ _make_positions() → positions tensor
    ├─ _make_input_tuple() → (table_idx_mapping, positions)
    ├─ _make_write_tuple() → (req_mapping, seq_lens)
    ├─ page_table[input_mapping] = out_loc
    └─ attn_backend.prepare_metadata(batch)
```

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| Token Budget | 限制单轮 prefill 的计算量，避免长 prompt 阻塞 |
| Chunked Prefill | 长 prompt 分多轮处理，每轮只计算一部分 |
| 前缀缓存复用 | 通过 `match_req` 复用已有 KV cache，减少重复计算 |
| 空间预留 | `reserved_size` 为正在 decode 的请求预留缓存空间 |
| 锁定机制 | `lock/unlock` 防止正在使用的缓存被驱逐 |

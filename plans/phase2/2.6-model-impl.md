# 2.6 模型实现分析

> 源文件：`python/minisgl/models/`

## 概览

mini-sGLang 的模型层采用注册表模式，支持 Llama、Qwen2、Qwen3、Qwen3-MoE 四种架构。所有模型继承自 `BaseLLMModel`，通过全局 Context 获取输入。

---

## 1. 模型注册机制

```python
# register.py
_MODEL_REGISTRY = {
    "LlamaForCausalLM":      (".llama",     "LlamaForCausalLM"),
    "Qwen2ForCausalLM":      (".qwen2",     "Qwen2ForCausalLM"),
    "Qwen3ForCausalLM":      (".qwen3",     "Qwen3ForCausalLM"),
    "Qwen3MoeForCausalLM":   (".qwen3_moe", "Qwen3MoeForCausalLM"),
}

def get_model_class(model_architecture: str, model_config: ModelConfig):
    module_path, class_name = _MODEL_REGISTRY[model_architecture]
    module = importlib.import_module(module_path, package=__package__)
    return getattr(module, class_name)(model_config)
```

使用延迟导入（`importlib`），只在需要时加载对应模型模块。

---

## 2. 基类设计

```python
# base.py
class BaseLLMModel(ABC, BaseOP):
    @abstractmethod
    def forward(self) -> torch.Tensor: ...
```

关键：`forward()` 无参数。输入通过 `get_global_ctx().batch.input_ids` 获取。这是为了兼容 CUDA 图捕获——图捕获要求函数签名固定。

---

## 3. ModelConfig - 模型配置

```python
@dataclass(frozen=True)
class ModelConfig:
    num_layers: int            # Transformer 层数
    num_qo_heads: int          # Query/Output 头数
    num_kv_heads: int          # KV 头数（GQA时 < num_qo_heads）
    head_dim: int              # 每个头的维度
    hidden_size: int           # 隐藏层维度
    vocab_size: int            # 词表大小
    intermediate_size: int     # FFN 中间层维度
    rms_norm_eps: float        # RMSNorm epsilon
    rotary_config: RotaryConfig  # RoPE 配置
    hidden_act: str            # 激活函数（如 "silu"）
    tie_word_embeddings: bool  # 是否共享 embedding 和 lm_head 权重
    # MoE 相关
    num_experts: int
    num_experts_per_tok: int
    moe_intermediate_size: int
    norm_topk_prob: bool
    model_type: str            # "llama", "qwen2", "qwen3", "qwen3_moe"
    architectures: list[str]
```

通过 `ModelConfig.from_hf(config)` 从 HuggingFace 配置转换，使用 `getattr` 兼容不同模型的配置差异。

---

## 4. Llama 模型实现（典型示例）

### 层级结构

```
LlamaForCausalLM (BaseLLMModel)
├── model: LlamaModel (BaseOP)
│   ├── embed_tokens: VocabParallelEmbedding
│   ├── layers: OPList[LlamaDecoderLayer]
│   │   └── LlamaDecoderLayer (BaseOP) × num_layers
│   │       ├── input_layernorm: RMSNormFused
│   │       ├── self_attn: RopeAttn (LlamaAttn)
│   │       ├── post_attention_layernorm: RMSNormFused
│   │       └── mlp: GatedMLP (LlamaMLP)
│   └── norm: RMSNormFused
└── lm_head: ParallelLMHead
```

### 前向传播流程

```python
# LlamaForCausalLM.forward()
def forward(self) -> torch.Tensor:
    output = self.model.forward(get_global_ctx().batch.input_ids)
    return self.lm_head.forward(output)

# LlamaModel.forward()
def forward(self, input_ids):
    x = self.embed_tokens.forward(input_ids)
    residual = None
    for layer in self.layers.op_list:
        x, residual = layer.forward(x, residual)
    return self.norm.forward(x, residual)[0]

# LlamaDecoderLayer.forward()
def forward(self, x, residual=None):
    x, residual = self.input_layernorm.forward(x, residual)   # fused RMSNorm
    x = self.self_attn.forward(x)                              # attention
    x, residual = self.post_attention_layernorm.forward(x, residual)
    x = self.mlp.forward(x)                                    # FFN
    return x, residual
```

### Residual 传递模式

注意 `residual` 在层间传递而非在层内相加。`RMSNormFused` 同时完成：
1. `output = RMSNorm(x + residual)`
2. `new_residual = x + residual`

这是一个 fused kernel 优化，减少了一次显存读写。

---

## 5. 关键层组件

| 组件 | 作用 | TP 支持 |
|------|------|---------|
| `VocabParallelEmbedding` | 词嵌入，按词表分片 | 列并行 |
| `ParallelLMHead` | 输出投影，支持 tie_word_embeddings | 列并行 |
| `RMSNormFused` | 融合的 RMSNorm + residual add | 无需分片 |
| `RopeAttn` | 带 RoPE 的注意力 | QO头按TP分片 |
| `GatedMLP` | SiLU 门控 FFN (gate_proj, up_proj, down_proj) | 列/行并行 |

---

## 6. 添加新模型的步骤

1. 在 `models/` 下创建新文件（如 `mistral.py`）
2. 继承 `BaseLLMModel`，实现 `forward()` 方法
3. 在 `register.py` 的 `_MODEL_REGISTRY` 中注册
4. 确保 `ModelConfig.from_hf()` 能正确解析新模型的 HF 配置

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| 无参 forward | 通过全局 Context 传入，兼容 CUDA 图 |
| 延迟导入 | 只加载需要的模型模块 |
| Fused RMSNorm | residual 在层间传递，减少显存访问 |
| TP 并行层 | Embedding/LMHead/Attn/MLP 都有并行版本 |
| frozen ModelConfig | 不可变配置，防止运行时意外修改 |

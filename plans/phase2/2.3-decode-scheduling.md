# 2.3 调度器 Decode 阶段分析

> 源文件：`python/minisgl/scheduler/decode.py`, `scheduler/scheduler.py`

## 概览

Decode 阶段负责自回归生成：每轮为每个活跃请求生成一个新 token。相比 Prefill 的"批量计算"，Decode 是"逐步生成"。

---

## 1. DecodeManager - 极简设计

```python
@dataclass
class DecodeManager:
    running_reqs: Set[Req] = field(default_factory=set)
```

整个 DecodeManager 只有一个 `Set[Req]`，设计极其精简。

### 核心方法

| 方法 | 作用 | 调用时机 |
|------|------|----------|
| `filter_reqs(reqs)` | 合并新请求，移除已完成的 | 每次 `_forward()` 后 |
| `remove_req(req)` | 从集合中移除单个请求 | 请求完成时 |
| `schedule_next_batch()` | 将所有活跃请求打包为 Batch | 调度主循环 |
| `inflight_tokens` | 所有活跃请求的剩余 token 总数 | Prefill 空间预留 |

### `filter_reqs` 详解

```python
def filter_reqs(self, reqs: Iterable[Req]) -> None:
    self.running_reqs = {
        req for req in self.running_reqs.union(reqs) if req.can_decode
    }
```

这一行做了三件事：
1. `union(reqs)` — 将刚完成 prefill 的请求加入
2. `if req.can_decode` — 过滤掉已完成的请求和 ChunkedReq
3. 重建集合 — 保持集合干净

---

## 2. Prefill → Decode 的转换

请求从 Prefill 进入 Decode 的关键路径：

```
Prefill Batch 执行完毕
    │
    ▼ Engine.forward_batch()
    │   → req.complete_one()  # cached_len = device_len, device_len += 1
    │
    ▼ Scheduler._forward()
    │   → decode_manager.filter_reqs(batch.reqs)
    │     → ChunkedReq.can_decode == False → 被过滤
    │     → 正常 Req.can_decode == True → 加入 running_reqs
    │
    ▼ 下一轮调度
        → decode_manager.schedule_next_batch()
        → Batch(reqs=list(running_reqs), phase="decode")
```

---

## 3. 调度优先级：Prefill First

```python
# scheduler.py:165
def _schedule_next_batch(self) -> ForwardInput | None:
    batch = (
        self.prefill_manager.schedule_next_batch(self.prefill_budget)
        or self.decode_manager.schedule_next_batch()
    )
    return self._prepare_batch(batch) if batch else None
```

当前策略是 **Prefill 优先**：
- 有待 prefill 的请求 → 执行 prefill batch
- 没有 prefill 请求 → 执行 decode batch
- 两者都没有 → 返回 None，调度器空闲

代码中有 `TODO: support other policies: e.g. DECODE first` 注释，说明未来可能支持其他策略。

---

## 4. Decode 结果处理

```python
# scheduler.py:77
def _process_last_data(self, last_data, ongoing_data):
    batch, (_, next_tokens_cpu, copy_done) = last_data[0].batch, last_data[1]
    copy_done.synchronize()    # 等待 GPU→CPU 拷贝完成

    for i, req in enumerate(batch.reqs):
        if req in self.finished_reqs or isinstance(req, ChunkedReq):
            continue

        next_token_id = next_tokens_cpu[i]
        req.append_host(next_token_id.unsqueeze(0))   # 追加到 input_ids

        finished = not req.can_decode                   # 达到最大长度
        if not req.sampling_params.ignore_eos:
            finished |= next_token == self.eos_token_id # 遇到 EOS

        reply.append(DetokenizeMsg(uid=req.uid, next_token=next_token, finished=finished))

        if finished:
            self.finished_reqs.add(req)
            self.decode_manager.remove_req(req)
```

### 资源释放

```python
    # 释放已完成且不在当前 batch 中的请求资源
    ongoing_reqs = ongoing_data[0].batch.reqs if ongoing_data else []
    for req in self.finished_reqs.difference(ongoing_reqs):
        self.table_manager.free(req.table_idx)
        self.cache_manager.free_and_cache_finished_req(
            req.cache_handle,
            req.input_ids[:req.cached_len],
            self.page_table[req.table_idx, :req.cached_len],
        )
```

注意：由于 overlap scheduling，已完成的请求可能仍在当前正在执行的 batch 中，所以需要 `difference(ongoing_reqs)` 排除。

---

## 5. Overlap Scheduling 与 Decode

Overlap scheduling 是 mini-sGLang 的核心优化，让 CPU 调度和 GPU 计算并行：

```
时间线:
─────────────────────────────────────────────────
GPU:  [  Batch N 执行  ] [  Batch N+1 执行  ]
CPU:       [ 处理 N-1 结果 + 调度 N+1 ]
─────────────────────────────────────────────────
```

```python
def overlap_loop(self, last_data):
    # 1. 接收新消息（非阻塞，如果有上一轮数据）
    for msg in self.receive_msg(blocking=...):
        self._process_one_msg(msg)

    # 2. 调度下一个 batch（CPU 工作）
    forward_input = self._schedule_next_batch()

    # 3. 在 engine stream 上执行（GPU 工作）
    if forward_input is not None:
        with self.engine_stream_ctx:
            self.engine.stream.wait_stream(self.stream)
            ongoing_data = (forward_input, self._forward(forward_input))

    # 4. 处理上一轮的结果（CPU 工作，与 GPU 并行）
    self._process_last_data(last_data, ongoing_data)
    return ongoing_data
```

双 CUDA Stream 设计：
- `self.stream` — 调度器的 stream，用于元数据准备
- `self.engine.stream` — 引擎的 stream，用于模型前向传播

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| Set 存储 | `running_reqs` 用 Set 实现 O(1) 的添加/删除/查找 |
| Prefill 优先 | 新请求尽快进入 decode，降低首 token 延迟 (TTFT) |
| 延迟释放 | overlap 模式下，资源释放需要排除正在执行的请求 |
| 双 Stream | 调度器和引擎使用不同 CUDA stream，实现 CPU/GPU 重叠 |

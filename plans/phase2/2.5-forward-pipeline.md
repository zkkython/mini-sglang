# 2.5 推理流程跟踪

> 源文件：`engine/engine.py`, `engine/graph.py`, `engine/sample.py`

## 概览

推理流程是从 Scheduler 调度出一个 Batch，到 Engine 执行前向传播并返回采样结果的完整路径。

```
Scheduler._forward(forward_input)
    │
    ├─ 从 token_pool 读取 input_ids
    ├─ Engine.forward_batch(batch, sample_args)
    │   ├─ CUDA图回放 或 正常forward
    │   ├─ req.complete_one()
    │   └─ Sampler.sample() → ForwardOutput
    │
    └─ 将 next_tokens 写回 token_pool
```

---

## 1. Scheduler._forward() - 调度器侧

```python
def _forward(self, forward_input: ForwardInput) -> ForwardOutput:
    batch, sample_args, input_mapping, output_mapping = forward_input

    # 1. 从 token_pool 读取 input_ids 到 batch
    batch.input_ids = self.token_pool[input_mapping]

    # 2. 调用引擎执行前向传播
    forward_output = self.engine.forward_batch(batch, sample_args)

    # 3. 将生成的 token 写回 token_pool
    self.token_pool[output_mapping] = forward_output.next_tokens_gpu

    # 4. 更新 decode 管理器
    self.decode_manager.filter_reqs(forward_input.batch.reqs)
    return forward_output
```

### token_pool 的读写

`token_pool` 是一个 `[max_running_req+1, max_seq_len]` 的 GPU 张量，存储每个请求的 token ids。

- **读取**：`token_pool[input_mapping]` 使用 2D 索引 `(table_idx_per_token, position_per_token)` 获取展平的 input_ids
- **写回**：`token_pool[output_mapping]` 使用 `(req_table_idx, next_position)` 写入新 token

---

## 2. Engine.forward_batch() - 引擎侧

```python
def forward_batch(self, batch, args) -> ForwardOutput:
    # 1. 设置全局 Context 的当前 batch
    with self.ctx.forward_batch(batch):
        # 2. 选择执行路径
        if self.graph_runner.can_use_cuda_graph(batch):
            logits = self.graph_runner.replay(batch)    # CUDA图
        else:
            logits = self.model.forward()               # 正常

    # 3. 更新所有请求的长度状态
    for req in batch.reqs:
        req.complete_one()

    # 4. 采样 + 异步拷贝
    next_tokens_gpu = self.sampler.sample(logits[:batch.size], args).to(torch.int32)
    next_tokens_cpu = next_tokens_gpu.to("cpu", non_blocking=True)
    copy_done_event = torch.cuda.Event()
    copy_done_event.record(self.stream)
    return ForwardOutput(next_tokens_gpu, next_tokens_cpu, copy_done_event)
```

---

## 3. CUDA 图：GraphRunner

### 何时使用 CUDA 图

```python
def can_use_cuda_graph(self, batch: Batch) -> bool:
    return batch.is_decode and batch.size <= self.max_graph_bs
```

仅 decode 阶段 + batch size 在预捕获范围内时使用。Prefill 因为序列长度不固定，无法使用 CUDA 图。

### 捕获过程（初始化时）

```python
# 预定义的 batch sizes: [1, 2, 4, 8, 16, 24, ..., max_bs]
for bs in sorted(graph_bs_list, reverse=True):  # 从大到小捕获
    graph = torch.cuda.CUDAGraph()
    batch = Batch(reqs=[dummy_req] * bs, phase="decode")
    # 预热
    self.buffer.logits[:bs] = model.forward()
    # 捕获
    with torch.cuda.graph(graph, pool=pool, stream=self.stream):
        self.buffer.logits[:bs] = model.forward()
    self.graph_map[bs] = graph
```

关键点：
- 从大到小捕获，复用 `pool` 减少显存碎片
- 每个 bs 捕获一个独立的 CUDAGraph
- 使用 `GraphCaptureBuffer` 作为固定的输入/输出缓冲区

### 回放过程

```python
def replay(self, batch: Batch) -> torch.Tensor:
    self.buffer.copy_from(batch)           # 拷贝实际数据到固定缓冲区
    g = self.graph_map[batch.padded_size]  # 查找对应 bs 的图
    self.attn_backend.prepare_for_replay(batch)
    g.replay()                             # 回放！
    return self.buffer.logits[:batch.size]
```

### Batch 填充（Padding）

```python
def pad_batch(self, batch: Batch) -> int:
    padded_size = next(bs for bs in self.graph_bs_list if bs >= batch.size)
    batch.padded_reqs = batch.reqs + [self.dummy_req] * (padded_size - batch.size)
    return batch.padded_size - batch.size
```

例如 batch.size=5 → padded_size=8，填充 3 个 dummy_req。

---

## 4. Sampler - 采样器

```python
@dataclass
class Sampler:
    device: torch.device
    vocab_size: int
```

### prepare() - 准备采样参数

```python
def prepare(self, batch) -> BatchSamplingArgs:
    params = [r.sampling_params for r in batch.reqs]
    if all(p.is_greedy for p in params):
        return BatchSamplingArgs(temperatures=None)  # 全贪心，无需温度

    # 否则构建 temperature/top_k/top_p 张量
    ...
```

### sample() - 执行采样

```python
def sample(self, logits, args) -> torch.Tensor:
    if args.temperatures is None:          # 贪心
        return torch.argmax(logits, dim=-1)
    return sample_impl(logits.float(), args.temperatures, args.top_k, args.top_p)
```

### sample_impl() - FlashInfer 采样

```python
def sample_impl(logits, temperatures, top_k, top_p):
    probs = flashinfer.sampling.softmax(logits, temperatures)
    # 根据参数组合选择采样方法
    if top_k and top_p:  return top_k_top_p_sampling_from_probs(probs, top_k, top_p)
    if top_k:            return top_k_sampling_from_probs(probs, top_k)
    if top_p:            return top_p_sampling_from_probs(probs, top_p)
    return sampling_from_probs(probs)
```

使用 FlashInfer 的 fused kernel 实现高效采样，避免多次 GPU kernel launch。

---

## 5. 完整推理时序图

```
Scheduler                          Engine                    GPU
    │                                │                        │
    │ _prepare_batch()               │                        │
    │  ├ allocate out_loc            │                        │
    │  ├ make positions              │                        │
    │  ├ make input/write tuples     │                        │
    │  └ prepare_metadata            │                        │
    │                                │                        │
    │ _forward()                     │                        │
    │  ├ read token_pool ──────────────────────────────────► read
    │  │                             │                        │
    │  ├ forward_batch() ──────────► │                        │
    │  │                             ├ set ctx.batch          │
    │  │                             ├ can_use_cuda_graph? ──► │
    │  │                             │  Yes: replay() ───────► graph.replay()
    │  │                             │  No:  model.forward() ► forward pass
    │  │                             │                        │
    │  │                             ├ complete_one() ×N      │
    │  │                             ├ sample() ─────────────► argmax/sampling
    │  │                             ├ to("cpu") ────────────► async copy
    │  │                             └ record event           │
    │  │                             │                        │
    │  ◄──── ForwardOutput ──────────┘                        │
    │  │                                                      │
    │  ├ write token_pool ────────────────────────────────► write
    │  └ filter_reqs()                                        │
    │                                                         │
    │ _process_last_data()                                    │
    │  ├ copy_done.synchronize() ◄────────────────────────── done
    │  ├ append_host() per req                                │
    │  ├ check finished                                       │
    │  └ send DetokenizeMsg                                   │
```

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| CUDA 图 | Decode 阶段固定计算图，消除 kernel launch 开销 |
| 异步拷贝 | GPU→CPU 使用 Event 机制，不阻塞 GPU 计算 |
| Fused 采样 | FlashInfer 的 softmax+sampling 融合 kernel |
| 全局 Context | 模型通过全局变量获取 batch，兼容 CUDA 图捕获 |
| 贪心快速路径 | 全贪心时跳过温度/top_k/top_p 计算 |

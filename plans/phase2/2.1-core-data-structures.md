# 2.1 核心数据结构分析

> 源文件：`python/minisgl/core.py`

## 概览

mini-sGLang 的核心数据结构定义在 `core.py` 中，共4个关键类：

```
SamplingParams  →  控制生成行为（温度、top_k、top_p等）
Req             →  单个推理请求的完整状态
Batch           →  一组请求的批处理容器
Context         →  全局推理上下文（注意力后端、当前batch）
```

---

## 1. SamplingParams - 采样参数

```python
@dataclass
class SamplingParams:
    temperature: float = 0.0      # 温度，0.0 = 贪心采样
    top_k: int = -1               # top-k 过滤，-1 = 不过滤
    top_p: float = 1.0            # top-p (nucleus) 过滤，1.0 = 不过滤
    ignore_eos: bool = False      # 是否忽略 EOS token
    max_tokens: int = 1024        # 最大生成 token 数
```

### 关键属性

- `is_greedy`: 当 `temperature <= 0` 或 `top_k == 1` 且 `top_p == 1.0` 时返回 True
- 贪心模式下采样直接用 `torch.argmax`，无需概率计算

### 设计要点

- 默认 `temperature=0.0` 意味着默认贪心采样，这是推理服务的常见默认值
- `top_k=-1` 表示不启用 top-k 过滤（在 Sampler 中会被转换为 vocab_size）

---

## 2. Req - 请求对象（核心）

```python
@dataclass(eq=False)
class Req:
    input_ids: torch.Tensor       # CPU tensor，完整的 token 序列
    table_idx: int                # 在 page_table 中的行索引
    cached_len: int               # 已缓存（KV cache已计算）的长度
    output_len: int               # 最大输出长度
    uid: int                      # 请求唯一标识
    sampling_params: SamplingParams
    cache_handle: BaseCacheHandle # KV缓存句柄
```

### 长度模型（最重要的概念）

```
|<--- cached_len --->|<--- extend_len --->|<--- remain_len --->|
|     已有KV缓存      |   本次需要计算的     |   还能生成的token    |
0                cached_len           device_len          max_device_len

device_len     = len(input_ids)           # 当前已有的总token数
max_device_len = len(input_ids) + output_len  # 最大可能的总长度
extend_len     = device_len - cached_len  # 本次前向传播需要处理的token数
remain_len     = max_device_len - device_len  # 还能继续生成的token数
```

### 生命周期方法

| 方法 | 作用 | 调用时机 |
|------|------|----------|
| `complete_one()` | `cached_len = device_len; device_len += 1` | Engine 前向传播后 |
| `append_host()` | 将新 token 追加到 `input_ids` | Scheduler 处理上一轮结果时 |
| `can_decode` | `remain_len > 0` | 判断是否还能继续生成 |

### `eq=False` 的设计意图

使用 `@dataclass(eq=False)` 意味着 Req 对象使用默认的 `id()` 比较（即对象身份比较），而非字段值比较。这是因为：
- Req 会被放入 `Set` 中（如 `DecodeManager.running_reqs`）
- 同一个 Req 对象在 prefill 和 decode 阶段间传递，需要保持身份一致性

### Req 状态转换图

```
UserMsg 到达
    │
    ▼
PendingReq (等待调度)
    │
    ▼ PrefillAdder.try_add_one()
Req / ChunkedReq (分配 table_idx + cache_handle)
    │
    ├─ ChunkedReq → 回到 PendingReq (部分prefill)
    │
    ▼ Prefill Batch 执行
Req (cached_len == device_len - 1, 进入 decode)
    │
    ▼ complete_one() + append_host() 循环
Req (remain_len == 0 或遇到 EOS → 完成)
    │
    ▼ 释放 table_idx + cache_handle
结束
```

---

## 3. Batch - 批处理容器

```python
@dataclass
class Batch:
    reqs: List[Req]                          # 请求列表
    phase: Literal["prefill", "decode"]      # 阶段标识
    # 以下字段由 scheduler 设置：
    input_ids: torch.Tensor                  # GPU tensor，展平的 token ids
    positions: torch.Tensor                  # 每个 token 的位置编码索引
    out_loc: torch.Tensor                    # 输出写入 page_table 的位置
    padded_reqs: List[Req]                   # 填充后的请求列表（CUDA图对齐）
    # 以下字段由 attention backend 设置：
    attn_metadata: BaseAttnMetadata          # 注意力计算的元数据
```

### Prefill vs Decode Batch 的区别

| 特征 | Prefill Batch | Decode Batch |
|------|--------------|--------------|
| `phase` | `"prefill"` | `"decode"` |
| 每个 Req 的 extend_len | 可能很大（整个prompt） | 恒为 1 |
| CUDA 图 | 不使用 | 可使用（`can_use_cuda_graph`） |
| 来源 | `PrefillManager` | `DecodeManager` |

### `padded_reqs` 的作用

CUDA 图要求固定的 batch size。`GraphRunner.pad_batch()` 会用 `dummy_req` 填充到最近的预定义 batch size（如 1, 2, 4, 8, 16, ...），`padded_reqs` 包含填充后的完整列表。

---

## 4. Context - 全局推理上下文

```python
@dataclass
class Context:
    page_size: int                           # 页大小（当前固定为1）
    attn_backend: BaseAttnBackend            # 注意力后端实例
    moe_backend: BaseMoeBackend              # MoE后端（可选）
    _batch: Batch | None                     # 当前正在处理的 batch
```

### 全局单例模式

```python
_GLOBAL_CTX: Context | None = None

def set_global_ctx(ctx: Context):    # Engine.__init__ 中调用一次
    ...
def get_global_ctx() -> Context:     # 模型 forward() 中调用
    ...
```

### `forward_batch` 上下文管理器

```python
@contextmanager
def forward_batch(self, batch: Batch):
    # 设置当前 batch → 执行前向传播 → 清除当前 batch
```

这个设计让模型的 `forward()` 方法不需要显式传入 batch 参数，而是通过全局 Context 获取：

```python
# LlamaForCausalLM.forward()
def forward(self) -> torch.Tensor:
    output = self.model.forward(get_global_ctx().batch.input_ids)  # 从全局获取
    return self.lm_head.forward(output)
```

这种"隐式传参"设计的好处是：CUDA 图捕获时，模型签名不变，只需替换 Context 中的 batch 即可。

---

## 5. 辅助数据结构

### PendingReq（`scheduler/utils.py`）

```python
@dataclass
class PendingReq:
    uid: int
    input_ids: torch.Tensor
    sampling_params: SamplingParams
    chunked_req: ChunkedReq | None = None   # 如果正在分块prefill
```

PendingReq 是 Req 的"前身"，在 `PrefillManager.pending_list` 中排队等待调度。当被 `PrefillAdder` 选中后，才会创建真正的 Req 对象。

### ForwardInput / ForwardOutput（`scheduler/scheduler.py`, `engine/engine.py`）

```python
class ForwardInput(NamedTuple):
    batch: Batch
    sample_args: BatchSamplingArgs
    input_tuple: Indice2D       # (token_mapping, positions)
    write_tuple: Indice2D       # (req_mapping, seq_lens)

class ForwardOutput(NamedTuple):
    next_tokens_gpu: torch.Tensor
    next_tokens_cpu: torch.Tensor
    copy_done_event: torch.cuda.Event
```

---

## 6. 数据结构关系图

```
UserMsg (来自 Tokenizer)
    │
    ▼
PendingReq ──────────────────────────────────────┐
    │                                             │
    ▼ PrefillAdder 分配资源                        │
Req ◄─── SamplingParams                          │
 │  ◄─── BaseCacheHandle (KV缓存句柄)             │
 │  ◄─── table_idx (page table 行号)              │
 │                                                │
 ▼                                                │
Batch ◄─── List[Req]                              │
 │    ◄─── phase ("prefill" / "decode")           │
 │    ◄─── attn_metadata (由注意力后端填充)         │
 │                                                │
 ▼                                                │
Context ◄─── attn_backend                         │
   │    ◄─── _batch (当前 Batch)                   │
   │                                              │
   ▼                                              │
Engine.forward_batch(batch)                       │
   │                                              │
   ▼                                              │
ForwardOutput ──► Scheduler._process_last_data()  │
   │                    │                         │
   │                    ▼                         │
   │              req.append_host(next_token)     │
   │              req.can_decode? ──No──► 释放资源  │
   │                    │Yes                      │
   │                    ▼                         │
   │              DecodeManager.running_reqs ─────┘
   ▼
DetokenizeMsg → Detokenizer → 用户
```

---

## 关键设计模式总结

| 模式 | 体现 | 目的 |
|------|------|------|
| 全局单例 | `_GLOBAL_CTX` | 让模型 forward 不依赖参数传递，兼容 CUDA 图 |
| 上下文管理器 | `Context.forward_batch()` | 安全地设置/清除当前 batch |
| 身份比较 | `Req(eq=False)` | 支持 Set 操作，保持对象唯一性 |
| 分层数据流 | PendingReq → Req → Batch | 不同阶段使用不同粒度的数据结构 |
| 延迟初始化 | Batch 的 `field(init=False)` 字段 | 由不同组件在不同阶段填充 |

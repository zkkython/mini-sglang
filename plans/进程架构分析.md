# Mini-SGLang 多进程架构分析

## 概述
mini-sGLang采用多进程架构设计，将不同的功能组件分离到独立的进程中，通过进程间通信（IPC）协调工作。这种设计提高了系统的模块化、稳定性和性能。

## 架构组件

### 1. 主进程（API Server）
**角色**：HTTP API服务器，接收客户端请求
**进程**：主Python进程（PID: 3029116）
**功能**：
- 解析命令行参数
- 启动子进程
- 提供OpenAI兼容的REST API
- 使用FastAPI + uvicorn作为Web框架

**关键文件**：
- `python/minisgl/__main__.py` - 主入口
- `python/minisgl/server/launch.py` - 启动逻辑
- `python/minisgl/server/api_server.py` - API服务器

### 2. Scheduler Workers
**角色**：调度和推理引擎
**数量**：每个GPU一个（Tensor Parallelism）
**命名**：`minisgl-TP{i}-scheduler`（i为GPU序号）

**功能**：
- 管理请求队列
- 执行模型推理（prefill和decode阶段）
- 管理KV缓存
- 协调Tensor Parallelism通信

**关键文件**：
- `python/minisgl/scheduler/scheduler.py` - 调度器实现
- `python/minisgl/engine/engine.py` - 推理引擎
- `python/minisgl/core.py` - 核心数据结构

### 3. Tokenizer Workers
**角色**：文本分词
**数量**：可配置（通过`--num-tokenizer`参数）
**命名**：`minisgl-tokenizer-{i}`

**功能**：
- 将文本转换为token ID
- 支持批处理
- 与前端API通信

**关键文件**：
- `python/minisgl/tokenizer/tokenize.py` - 分词逻辑
- `python/minisgl/tokenizer/server.py` - tokenizer服务器

### 4. Detokenizer Worker
**角色**：文本反分词
**数量**：1个（固定）
**命名**：`minisgl-detokenizer-0`

**功能**：
- 将token ID转换回文本
- 流式输出支持
- 与前端API通信

**关键文件**：
- `python/minisgl/tokenizer/detokenize.py` - 反分词逻辑

## 进程启动流程分析

### 启动顺序（`launch.py:40-113`）
```python
def launch_server(run_shell: bool = False) -> None:
    # 1. 解析参数
    server_args, run_shell = parse_args(sys.argv[1:], run_shell)

    # 2. 定义子进程启动函数
    def start_subprocess() -> None:
        mp.set_start_method("spawn", force=True)  # 使用spawn方法

        # 3. 启动Scheduler Workers（每个GPU）
        for i in range(world_size):
            mp.Process(target=_run_scheduler, name=f"minisgl-TP{i}-scheduler").start()

        # 4. 启动Detokenizer Worker
        mp.Process(target=tokenize_worker, name="minisgl-detokenizer-0").start()

        # 5. 启动Tokenizer Workers
        for i in range(num_tokenizers):
            mp.Process(target=tokenize_worker, name=f"minisgl-tokenizer-{i}").start()

        # 6. 等待所有worker就绪
        for _ in range(num_tokenizers + 2):
            logger.info(ack_queue.get())

    # 7. 运行API服务器
    run_api_server(server_args, start_subprocess, run_shell=run_shell)
```

### Scheduler初始化（`_run_scheduler`函数）
```python
def _run_scheduler(args: ServerArgs, ack_queue: mp.Queue[str]) -> None:
    import torch
    from minisgl.scheduler import Scheduler

    with torch.inference_mode():
        scheduler = Scheduler(args)  # 初始化调度器
        scheduler.sync_all_ranks()   # 同步所有rank

        if args.tp_info.is_primary():
            ack_queue.put("Scheduler is ready")  # 发送就绪信号

        scheduler.run_forever()  # 进入主循环
```

## 进程间通信机制

### 1. ZMQ（ZeroMQ）通信
**用途**：高性能进程间消息传递
**组件**：
- `ZmqAsyncPushQueue` - 异步推送队列
- `ZmqAsyncPullQueue` - 异步拉取队列

**地址配置**（通过`ServerArgs`）：
- `zmq_tokenizer_addr` - tokenizer地址
- `zmq_detokenizer_addr` - detokenizer地址
- `zmq_backend_addr` - 后端地址
- `zmq_frontend_addr` - 前端地址

### 2. Multiprocessing Queue
**用途**：进程启动同步
**组件**：`mp.Queue[str]`
**功能**：确保所有子进程就绪后才启动API服务器

### 3. TCP通信（PyTorch分布式）
**用途**：Tensor Parallelism通信
**端口**：自动分配（如1920）
**问题**：端口冲突可能导致启动失败

## 进程树示例
基于实际运行的进程观察：
```
python(3029116)-+-python(3029374)  # resource_tracker
                |-python(3029375)  # Scheduler Worker 1
                |-python(3029376)  # Scheduler Worker 2（如果有多个GPU）
```

## 配置参数影响

### 影响进程数量的参数：
1. `--tp-size` 或 `--tensor-parallel-size`：Scheduler Workers数量
2. `--num-tokenizer`：Tokenizer Workers数量
3. `--max-running-requests`：最大并发请求数

### 默认配置：
- TP size = 1（单GPU）
- num_tokenizer = 0（与detokenizer共享）
- max_running_req = 256

## 问题诊断

### 常见启动问题：
1. **端口冲突**：Torch分布式通信端口被占用
   - 解决方案：使用不同端口或清理占用进程

2. **GLIBCXX版本不匹配**：系统库版本过旧
   - 解决方案：使用系统Python创建虚拟环境

3. **模型下载失败**：网络连接问题
   - 解决方案：使用本地模型路径

### 调试技巧：
1. **查看进程树**：`pstree -p <主进程PID>`
2. **检查端口占用**：`ss -tlnp | grep :<端口>`
3. **启用详细日志**：`LOG_LEVEL=DEBUG`
4. **监控资源使用**：`nvidia-smi`, `htop`

## 性能优化考虑

### 进程数量调优：
1. **Tokenizer Workers**：根据请求并发量调整
2. **Scheduler Workers**：根据GPU数量设置
3. **线程池大小**：影响CPU密集型操作

### 内存管理：
1. **KV缓存比例**：通过`--memory-ratio`控制
2. **批处理大小**：影响GPU内存使用
3. **序列长度**：影响显存需求

## 扩展性设计

### 水平扩展：
1. **Tensor Parallelism**：跨多个GPU扩展模型
2. **多实例部署**：多个mini-sGLang实例负载均衡

### 垂直扩展：
1. **更大批处理**：提高GPU利用率
2. **更长序列**：支持长上下文

## 学习要点总结

### 已掌握：
1. ✅ 多进程架构的基本组成
2. ✅ 进程启动流程和同步机制
3. ✅ 进程间通信方式（ZMQ、Queue、TCP）
4. ✅ 配置参数对架构的影响

### 待深入学习：
1. 🔄 请求处理完整流程
2. 🔄 ZMQ消息格式和协议
3. 🔄 调度器的prefill/decode策略
4. 🔄 KV缓存管理机制

## 下一步学习计划
基于当前分析，继续任务1.4：跟踪请求生命周期，理解完整的请求处理流程。
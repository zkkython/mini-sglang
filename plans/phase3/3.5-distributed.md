# 3.5 分布式通信分析

> 源文件：`python/minisgl/distributed/`, `python/minisgl/layers/`

## 概览

mini-sGLang 的分布式通信服务于 Tensor Parallelism (TP)，采用插件式设计支持两种后端：

```
DistributedCommunicator (插件式分发)
    ├─ TorchDistributedImpl    (PyTorch 原生 NCCL)
    └─ PyNCCLDistributedImpl   (自定义 PyNCCL via TVM-FFI)
```

---

## 1. DistributedInfo - TP 信息

```python
@dataclass(frozen=True)
class DistributedInfo:
    rank: int    # 当前进程的 TP 排名
    size: int    # TP 总进程数

    def is_primary(self) -> bool:
        return self.rank == 0
```

全局单例，通过 `set_tp_info()` 在 Engine 初始化时设置，通过 `get_tp_info()` 全局访问。

---

## 2. DistributedCommunicator - 通信分发器

```python
class DistributedCommunicator:
    plugins: List[DistributedImpl] = [TorchDistributedImpl()]

    def all_reduce(self, x: torch.Tensor) -> torch.Tensor:
        return self.plugins[-1].all_reduce(x)    # 使用最后一个插件

    def all_gather(self, x: torch.Tensor) -> torch.Tensor:
        return self.plugins[-1].all_gather(x)
```

设计特点：
- **类变量 `plugins`**：所有实例共享同一个插件列表
- **后进先用**：`plugins[-1]` 意味着后注册的插件优先级更高
- 初始只有 `TorchDistributedImpl`，启用 PyNCCL 后 `append` 新插件

### 两种后端

| 后端 | 实现 | 优势 |
|------|------|------|
| `TorchDistributedImpl` | `torch.distributed.all_reduce/all_gather` | 简单，依赖 PyTorch |
| `PyNCCLDistributedImpl` | 自定义 NCCL wrapper via TVM-FFI | 更灵活，可控缓冲区 |

---

## 3. PyNCCL 初始化

```python
def enable_pynccl_distributed(tp_info, tp_cpu_group, max_bytes):
    if tp_info.size == 1:
        return                    # 单 GPU 不需要通信

    comm = init_pynccl(
        tp_rank=tp_info.rank,
        tp_size=tp_info.size,
        tp_cpu_group=tp_cpu_group,
        max_size_bytes=max_bytes,  # 预分配通信缓冲区
    )

    DistributedCommunicator.plugins.append(PyNCCLDistributedImpl(comm))
```

### init_pynccl() 详解

```python
def init_pynccl(*, tp_rank, tp_size, tp_cpu_group, max_size_bytes):
    module = _load_nccl_module()      # AOT 编译 pynccl.cu
    cls = _get_pynccl_wrapper_cls()   # 注册 TVM-FFI 对象

    # 通过 Gloo 广播 NCCL unique ID
    if tp_rank == 0:
        id_list = [module.create_nccl_uid()]
        torch.distributed.broadcast_object_list(id_list, src=0, group=tp_cpu_group)
    else:
        id_list = [None]
        torch.distributed.broadcast_object_list(id_list, src=0, group=tp_cpu_group)

    nccl_id = id_list[0]
    return cls(tp_rank, tp_size, max_size_bytes, nccl_id)
```

关键步骤：
1. AOT 编译 CUDA C++ 代码 (`pynccl.cu`)
2. Rank 0 创建 NCCL unique ID
3. 通过 Gloo CPU group 广播 ID 到所有 rank
4. 每个 rank 用 ID 初始化本地 NCCL communicator

---

## 4. Tensor Parallelism 通信模式

### 4.1 Embedding 层

```python
# VocabParallelEmbedding
class VocabParallelEmbedding(BaseOP):
    def forward(self, input_ids):
        y = indexing(self.weight, input_ids, vocab_range=(self.start, self.length))
        return self._comm.all_reduce(y) if self.tp_size > 1 else y
```

词表按列分片，每个 rank 只持有 `vocab_size / tp_size` 的词向量。查找后 `all_reduce` 合并。

### 4.2 Linear 层

```python
# LinearRowParallel (MLP 的 down_proj)
class LinearRowParallel(BaseOP):
    def forward(self, x):
        y = F.linear(x, self.weight)
        return self._comm.all_reduce(y) if self.tp_size > 1 else y

# LinearOProj (Attention 的 output projection)
class LinearOProj(BaseOP):
    def forward(self, x):
        y = F.linear(x, self.weight)
        return self._comm.all_reduce(y) if self.tp_size > 1 else y
```

TP 中，线性层的分片策略：

```
列并行 (Column Parallel):
  权重 W 按列分为 [W_0, W_1, ..., W_{tp-1}]
  每个 rank 计算 y_i = x @ W_i
  不需要通信（各自独立）

行并行 (Row Parallel):
  权重 W 按行分为 [W_0; W_1; ...; W_{tp-1}]
  每个 rank 计算 y_i = x_i @ W_i
  需要 all_reduce: y = sum(y_0, y_1, ..., y_{tp-1})
```

### 4.3 LM Head

```python
# ParallelLMHead
class ParallelLMHead(BaseOP):
    def forward(self, x):
        logits = F.linear(x, self.weight)
        output_tensor = self._comm.all_gather(logits)  # 收集完整 logits
        return output_tensor
```

LM Head 使用 `all_gather` 而非 `all_reduce`，因为需要完整的 vocab_size 维度的 logits 来进行采样。

### 4.4 MoE 层

```python
class MoELayer(BaseOP):
    def forward(self, hidden_states, router_logits):
        final_hidden_states = ctx.moe_backend.forward(...)
        if self.tp_size > 1:
            final_hidden_states = self._comm.all_reduce(final_hidden_states)
        return final_hidden_states
```

MoE 的专家权重按列分片（intermediate_size / tp_size），输出后 `all_reduce`。

---

## 5. 通信拓扑总览

```
一次 Transformer 层的通信:

Embedding:       all_reduce  (vocab 分片 → 合并)
    │
    ▼
Attention:
  QKV Linear:    无通信 (列并行)
  Attention:     无通信 (本地计算)
  O Linear:      all_reduce  (行并行 → 合并)
    │
    ▼
MLP (Dense):
  Gate/Up:       无通信 (列并行)
  Down:          all_reduce  (行并行 → 合并)
    │
    ▼
MLP (MoE):
  Expert MLPs:   无通信 (列并行)
  Output:        all_reduce  (行并行 → 合并)
    │
    ▼
LM Head:         all_gather  (收集完整 logits)
```

每层有 **2 次 all_reduce**（Attention O-proj + MLP down-proj），加上首尾各 1 次。

---

## 6. Engine 中的通信初始化

```python
# engine.py
def _init_communication(self, config):
    if config.tp_info.size == 1 or config.use_pynccl:
        # Gloo 控制通信 + PyNCCL 数据通信
        torch.distributed.init_process_group(backend="gloo", ...)
        enable_pynccl_distributed(tp_info, tp_cpu_group, max_bytes)
    else:
        # NCCL 数据通信 + Gloo CPU 通信
        torch.distributed.init_process_group(backend="nccl", ...)
        tp_cpu_group = torch.distributed.new_group(backend="gloo")
```

两种模式：
- **Gloo + PyNCCL**（默认）：Gloo 用于 CPU 同步（broadcast ID、同步显存），PyNCCL 用于 GPU 数据通信
- **原生 NCCL**：全部使用 PyTorch NCCL，额外创建 Gloo group 用于 CPU 操作

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| 插件式通信 | `DistributedCommunicator.plugins` 列表，后注册优先 |
| 双 backend | Gloo (CPU同步) + NCCL/PyNCCL (GPU数据) |
| TVM-FFI | PyNCCL 通过 TVM-FFI 调用 C++ NCCL API |
| 缓冲区预分配 | `max_size_bytes` 预分配通信缓冲区，避免运行时分配 |
| 按需通信 | `tp_size == 1` 时所有通信操作跳过 |

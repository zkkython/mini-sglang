# 3.3 FlashInfer 实现分析

> 源文件：`python/minisgl/attention/fi.py`

## 概览

FlashInfer 后端使用 `flashinfer` 库，采用 wrapper + plan 两阶段 API。与 FA 的核心区别在于：使用 1D ragged indices（而非 2D page_table），以及显式的 `plan()` + `run()` 分离。

---

## 1. FlashInferBackend 初始化

```python
class FlashInferBackend(BaseAttnBackend):
    def __init__(self, config, kvcache, page_table):
        # 128MB 浮点工作空间
        self.float_workspace_buffer = torch.empty(
            128 * 1024 * 1024, dtype=torch.uint8, device=device
        )
        # Prefill Wrapper
        self.prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(
            self.float_workspace_buffer,
            kv_layout="NHD",
            backend="fa2",        # flashinfer 的 fa3 慢，用 fa2
        )
        # Decode Wrapper
        self.decode_wrappers = BatchDecodeWithPagedKVCacheWrapper(
            self.float_workspace_buffer,
            use_tensor_cores=self.use_tensor_cores,
            kv_layout="NHD",
            backend="fa2",
        )
        # 复用 int workspace buffer
        self.int_workspace_buffer = self.prefill_wrapper._int_workspace_buffer
        self.decode_wrappers._int_workspace_buffer = self.int_workspace_buffer
```

关键设计：
- **workspace buffer 共享**：prefill 和 decode 共享同一个 `float_workspace_buffer` 和 `int_workspace_buffer`，节省显存
- **`backend="fa2"`**：FlashInfer 内部使用 FA2 作为后端（注释说 fa3 性能不佳）
- **NHD layout**：KV cache 按 `[num_pages, num_heads, head_dim]` 排列

---

## 2. Tensor Core 使用决策

```python
@cached_property
def use_tensor_cores(self) -> bool:
    if ENV.FLASHINFER_USE_TENSOR_CORES.value is not None:
        return overriden_value    # 环境变量覆盖
    GQA = num_qo_heads // num_kv_heads
    return GQA >= 4              # GQA ratio >= 4 时启用
```

当 GQA（Grouped Query Attention）比率 >= 4 时，使用 Tensor Core 加速 decode。这是因为高 GQA 比率下，每个 KV head 对应多个 Q head，Tensor Core 的矩阵运算更有优势。

---

## 3. FIMetadata - 元数据结构

```python
@dataclass
class FIMetadata(BaseAttnMetadata):
    cu_seqlens_q_cpu:   torch.Tensor   # CPU: Q 累积长度
    cu_seqlens_k_cpu:   torch.Tensor   # CPU: KV 累积长度
    cu_seqlens_q_gpu:   torch.Tensor   # GPU: Q 累积长度（用于 get_last_indices）
    indices:            torch.Tensor   # GPU: 1D ragged 页面索引
    last_page_len_cpu:  torch.Tensor   # CPU: 每个请求最后一页长度
    num_qo_heads:       int
    num_kv_heads:       int
    head_dim:           int
    page_size:          Literal[1]     # 固定为 1
    pos_encoding_mode:  str            # "NONE"（RoPE 在模型层处理）
    seq_lens_cpu:       torch.Tensor   # CPU: 序列长度
    dtype:              torch.dtype
    wrapper:            Wrapper        # prefill 或 decode wrapper
    initialized:        bool = False   # plan() 是否已调用
```

### 1D Ragged Indices vs 2D Page Table

FlashInfer 使用 1D 展平的索引，而非 FA 的 2D 子页表：

```
FA:  page_table = [[p0, p1, p2, 0, 0],   # req0: 3 pages
                   [p3, p4, p5, p6, p7]]  # req1: 5 pages

FI:  indices = [p0, p1, p2, p3, p4, p5, p6, p7]  # 展平的 ragged tensor
     cu_seqlens_k = [0, 3, 8]                     # 用 indptr 标记边界
```

这种设计更紧凑，无需填充到统一长度。

---

## 4. prepare_metadata() - 元数据准备

```python
def prepare_metadata(self, batch):
    seqlens_q = [req.extend_len for req in reqs]
    seqlens_k = [req.device_len for req in reqs]

    seq_len_cpu = torch.tensor(seqlens_k, ...)
    cu_seqlens_k_cpu = cumsum([0] + seqlens_k)

    # Q 累积长度: 同 FA 的三路径逻辑
    if max_seqlen_q == 1:           cu_seqlens_q_cpu = arange(...)
    elif all(cached==0):            cu_seqlens_q_cpu = cu_seqlens_k_cpu
    else:                           cu_seqlens_q_cpu = cumsum([0] + seqlens_q)

    # 关键区别: 1D ragged indices
    indices = torch.cat([
        page_table[req.table_idx, :req.device_len] for req in reqs
    ])

    # last_page_len: page_size=1 时恒为 1
    last_page_len_cpu = self._get_ones_cpu(padded_size)

    # 根据 batch 阶段选择 wrapper
    wrapper = self.decode_wrappers if batch.is_decode else self.prefill_wrapper
```

---

## 5. Plan + Run 两阶段 API

### _initialize_metadata_once() - 延迟 Plan

```python
@staticmethod
def _initialize_metadata_once(metadata: FIMetadata) -> None:
    if metadata.initialized:
        return
    metadata.initialized = True

    if isinstance(metadata.wrapper, BatchDecodeWithPagedKVCacheWrapper):
        metadata.wrapper.plan(
            indptr=metadata.cu_seqlens_k_cpu,
            indices=metadata.indices,
            last_page_len=metadata.last_page_len_cpu,
            num_qo_heads=..., num_kv_heads=..., head_dim=...,
            page_size=1,
            non_blocking=True,    # 异步执行
        )
    else:  # Prefill
        metadata.wrapper.plan(
            qo_indptr=metadata.cu_seqlens_q_cpu,
            paged_kv_indptr=metadata.cu_seqlens_k_cpu,
            paged_kv_indices=metadata.indices,
            causal=True,
            non_blocking=True,
        )
```

`plan()` 做什么：
1. 计算注意力的内部调度方案（如何分配 thread blocks）
2. 预处理页面索引
3. `non_blocking=True` 允许与其他 GPU 操作重叠

### forward() - 执行

```python
def forward(self, q, k, v, layer_id, batch):
    metadata = batch.attn_metadata
    self._initialize_metadata_once(metadata)              # 确保 plan 已执行
    self.kvcache.store_kv(k, v, batch.out_loc, layer_id)  # 存储 KV
    kv_cache = (self.kvcache.k_cache(layer_id), self.kvcache.v_cache(layer_id))
    return metadata.wrapper.run(q=q, paged_kv_cache=kv_cache)   # 执行注意力
```

`plan()` 在第一层的 `forward()` 中调用一次，后续层复用（因为页面映射不变）。

---

## 6. CUDA 图支持

FlashInfer 的 CUDA 图支持使用专用的 `CUDAGraphBatchDecodeWithPagedKVCacheWrapper`：

### 初始化

```python
def init_capture_graph(self, max_seq_len, bs_list):
    capture = FICaptureData.create(max_bs, max_seq_len, device)
    capture.page_table = capture.page_table.view(-1)  # 展平为 1D ragged
```

### 捕获准备

```python
def prepare_for_capture(self, batch):
    # 为每个 bs 创建专用的 CUDAGraphBatchDecodeWrapper
    self.graph_wrappers[bs] = CUDAGraphBatchDecodeWithPagedKVCacheWrapper(
        self.float_workspace_buffer,
        kv_layout="NHD",
        use_tensor_cores=self.use_tensor_cores,
        indptr_buffer=capture.cu_seqlens_k[:bs+1],     # 固定地址
        indices_buffer=capture.indices,                  # 固定地址
        last_page_len_buffer=capture.one_tensor[:bs],   # 固定地址
    )
    # 立即调用 plan + run 预热
    self.prepare_metadata(batch)
    self._initialize_metadata_once(metadata)
```

### 回放准备

```python
def prepare_for_replay(self, batch):
    metadata.wrapper = self.graph_wrappers[bs]
    self._initialize_metadata_once(metadata)  # 重新 plan（更新调度）
```

关键区别：FA 是拷贝数据到固定缓冲区，FI 是重新调用 `plan()` 但使用固定地址的缓冲区。

---

## 7. FlashInfer vs FlashAttention 详细对比

| 维度 | FlashAttention | FlashInfer |
|------|---------------|-----------|
| **KV 索引** | 2D `[bs, max_seq_len]` | 1D ragged + indptr |
| **API 模式** | 单次调用 `flash_attn_with_kvcache` | `plan()` + `run()` 两阶段 |
| **Wrapper** | 无 | BatchPrefill/BatchDecode Wrapper |
| **CUDA 图** | 直接 copy_ 元数据 | 专用 CUDAGraphWrapper + re-plan |
| **Tensor Core** | 不支持 | GQA >= 4 时启用 |
| **Workspace** | 无需预分配 | 128MB float + int workspace |
| **Plan 开销** | 无 | 有（但可 non_blocking） |
| **最适场景** | Prefill（长序列） | Decode（短 Q + Tensor Core） |

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| 延迟 plan | `_initialize_metadata_once` 确保 plan 在第一层 forward 时执行一次 |
| Workspace 共享 | prefill/decode 共享 float/int workspace，节省显存 |
| 1D Ragged | 无填充的紧凑索引，适合不等长序列 |
| per-bs Wrapper | CUDA 图为每个 batch size 创建独立的 CUDAGraphWrapper |
| `backend="fa2"` | FlashInfer 内部使用 FA2，因为 FA3 当前性能不佳 |

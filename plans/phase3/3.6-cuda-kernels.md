# 3.6 CUDA 内核分析

> 源文件：`python/minisgl/kernel/`

## 概览

mini-sGLang 的 CUDA 内核通过两种方式编译和加载：

| 方式 | 函数 | 用途 | 特点 |
|------|------|------|------|
| JIT | `load_jit()` | index, store | 模板参数化，按需编译 |
| AOT | `load_aot()` | pynccl, radix, tensor | 预编译，依赖外部库 |

底层均使用 **TVM-FFI** (`tvm_ffi.cpp.load` / `load_inline`) 进行编译和 FFI 绑定。

---

## 1. 编译基础设施 (kernel/utils.py)

### KernelConfig

```python
class KernelConfig(NamedTuple):
    num_threads: int       # CUDA 线程数
    max_occupancy: int     # 最大占用率
    use_pdl: bool          # 是否使用 PDL (Programmatic Dependent Launch)
```

### JIT 编译: load_jit()

```python
def load_jit(*args, cuda_files, cuda_wrappers, ...):
    # 1. 从 kernel/jit/ 目录读取 CUDA 源文件
    cuda_paths = [KERNEL_PATH / "jit" / f for f in cuda_files]
    cuda_sources = [f'#include "{path}"' for path in cuda_paths]

    # 2. 生成 TVM-FFI 导出包装器
    cuda_sources += [_make_wrapper(tup) for tup in cuda_wrappers]
    # 例: TVM_FFI_DLL_EXPORT_TYPED_FUNC(launch, (IndexKernel<128,1,false>::run));

    # 3. 调用 tvm_ffi.cpp.load_inline 编译
    return load_inline(
        _make_name(*args),           # 唯一名称，如 "minisgl__index_512_4"
        cuda_sources=cuda_sources,
        extra_cflags=["-std=c++20", "-O3"],
        extra_cuda_cflags=["-std=c++20", "-O3", "--expt-relaxed-constexpr"],
    )
```

JIT 编译使用 C++ 模板参数化，**同一个内核模板针对不同参数生成不同的特化版本**。

### AOT 编译: load_aot()

```python
def load_aot(*args, cpp_files=None, cuda_files=None, extra_ldflags=None, ...):
    # 从 kernel/csrc/src/ 目录加载预写好的源文件
    cpp_files = [str(KERNEL_PATH / "src" / f) for f in cpp_files]
    return tvm_ffi.cpp.load(
        _make_name(*args),
        cpp_files=cpp_files,
        cuda_files=cuda_files,
        extra_ldflags=extra_ldflags,  # 如 ["-lnccl"]
    )
```

AOT 编译不使用模板参数，适合固定接口的内核。

---

## 2. Indexing 内核 (index.py)

用途：高效的 Embedding 查找（替代 `torch.embedding`）。

```python
def indexing(weights, indices, *, output=None, vocab_range=None):
    element_size = weights.shape[1] * weights.element_size()

    # 根据元素大小选择分割数（优化内存访问）
    if element_size % 2048 == 0:   num_splits = 4
    elif element_size % 1024 == 0: num_splits = 2
    else:                          num_splits = 1

    module = _jit_index_module(element_size, num_splits=num_splits)
    module.launch(weights, indices, output, vocab_range)
    return output
```

- `num_splits`：将每行数据分成多个 CUDA block 处理，提高并行度
- `@functools.cache`：相同参数只编译一次
- `vocab_range`：支持 TP 分片词表的范围映射

---

## 3. Store Cache 内核 (store.py)

用途：将 K、V 张量散列写入 KV cache 的指定页面位置。

```python
def store_cache(k_cache, v_cache, indices, k, v):
    num_tokens = k_cache.shape[0]
    k_cache = k_cache.view(num_tokens, -1)   # 展平为 [num_pages, kv_heads*head_dim]
    v_cache = v_cache.view(num_tokens, -1)
    element_size = k_cache.shape[1] * k_cache.element_size()
    module = _jit_store_module(element_size)
    module.launch(k_cache, v_cache, indices, k, v)
```

操作本质是：`k_cache[indices] = k; v_cache[indices] = v`，但用 CUDA 内核实现以支持高效的散列写入。

---

## 4. PyNCCL 内核 (pynccl.py)

用途：封装 NCCL 的 `all_reduce` 和 `all_gather` 操作。

```python
def init_pynccl(*, tp_rank, tp_size, tp_cpu_group, max_size_bytes):
    module = _load_nccl_module()          # AOT 编译 pynccl.cu
    cls = _get_pynccl_wrapper_cls()       # 注册 TVM-FFI 对象类型

    # 广播 NCCL unique ID
    if tp_rank == 0:
        id_list = [module.create_nccl_uid()]
    else:
        id_list = [None]
    torch.distributed.broadcast_object_list(id_list, src=0, group=tp_cpu_group)

    return cls(tp_rank, tp_size, max_size_bytes, id_list[0])
```

TVM-FFI 对象注册：

```python
@tvm_ffi.register_object("minisgl.NCCLWrapper")
class PyNCCLImpl(tvm_ffi.Object):
    def __init__(self, *args):
        self.__ffi_init__(*args)
    # 方法通过 FFI 自动绑定: all_reduce, all_gather, get_buffer
```

---

## 5. Radix 内核 (radix.py)

用途：快速比较两个 1D int CPU tensor（用于 RadixTree 的键匹配）。

```python
def fast_compare_key(x: torch.Tensor, y: torch.Tensor) -> int:
    return _load_radix_module().fast_compare_key(x, y)
```

使用 AOT 编译的 C++ 实现 (`radix.cpp`)，比 Python 循环比较快得多。返回首个不同位置的索引。

---

## 6. MoE Triton 内核 (moe_impl.py + triton/fused_moe.py)

### fused_moe_kernel - 融合 MoE 矩阵乘法

```python
@triton.jit
def fused_moe_kernel(
    a_ptr, b_ptr, c_ptr,           # A: tokens, B: expert weights, C: output
    topk_weights_ptr,               # 路由权重
    sorted_token_ids_ptr,           # 按专家排序的 token 索引
    expert_ids_ptr,                 # 每个 block 对应的专家 ID
    num_tokens_post_padded_ptr,
    N, K, EM, num_valid_tokens,     # 维度信息
    ...
    BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M,  # 分块参数
    MUL_ROUTED_WEIGHT,              # 是否乘路由权重
):
    # 1. 通过 sorted_token_ids 确定当前 block 处理的 tokens
    # 2. 通过 expert_ids 确定使用哪个专家的权重
    # 3. 执行分块矩阵乘法 (BLOCK_SIZE_M × BLOCK_SIZE_N)
    # 4. 可选乘以路由权重
```

### FusedMoe 完整流程

```
hidden_states
    │
    ▼ fused_topk()                    # SGL kernel: softmax + top-k
    │   → topk_weights, topk_ids
    │
    ▼ moe_align_block_size()          # SGL kernel: 对齐 block size
    │   → sorted_token_ids, expert_ids
    │
    ▼ fused_moe_kernel_triton()       # Triton: hidden × gate_up_proj
    │   → intermediate_cache1
    │
    ▼ silu_and_mul()                  # 激活函数
    │   → intermediate_cache2
    │
    ▼ fused_moe_kernel_triton()       # Triton: activated × down_proj
    │   → intermediate_cache3
    │
    ▼ moe_sum_reduce_triton()         # Triton: 按 topk 求和
    │   → output
    │
    ▼ all_reduce()                    # TP 通信
    │
    ▼ final_output
```

### moe_sum_reduce_kernel - 求和归约

```python
@triton.jit
def moe_sum_reduce_kernel(input_ptr, output_ptr, token_num, topk_num, hidden_dim, ...):
    # 对每个 token, 将 topk 个专家的输出求和
    for token_index in range(token_start, token_end):
        accumulator = zeros(BLOCK_DIM)
        for i in range(topk_num):
            accumulator += load(input[token_index, i, :])
        store(output[token_index, :], accumulator)
```

---

## 7. 内核编译缓存

所有 JIT/AOT 内核都使用 `@functools.cache` 装饰器：

```python
@functools.cache
def _jit_index_module(element_size, *, num_splits=1, config=DEFAULT_CONFIG):
    return load_jit("index", ...)
```

**同一参数组合只编译一次**，后续调用直接返回缓存的模块。这意味着首次调用有编译开销，后续调用接近零开销。

---

## 8. __main__.py - 开发工具

```python
# python -m minisgl.kernel
def generate_clangd():
    # 生成 .clangd 配置文件，用于 IDE 的 CUDA 代码补全和分析
    # 自动检测 GPU compute capability
    # 包含 TVM-FFI 和 DLPack 的头文件路径
```

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| TVM-FFI | 统一的 C++/CUDA FFI 框架，支持 JIT 和 AOT |
| 模板特化 | JIT 内核通过 C++ 模板参数化，针对不同配置生成最优代码 |
| functools.cache | 编译结果缓存，避免重复编译 |
| Triton MoE | MoE 使用 Triton JIT 编译，灵活的分块配置 |
| 双编译路径 | 简单内核用 JIT (灵活)，复杂/有外部依赖的用 AOT |

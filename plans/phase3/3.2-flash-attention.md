# 3.2 FlashAttention 实现分析

> 源文件：`python/minisgl/attention/fa.py`

## 概览

FlashAttention 后端基于 `sgl_kernel.flash_attn`（即 SGLang 优化的 FlashAttention 实现），使用 FA3 算法（Hopper 架构优化）。它通过 2D page_table 管理 KV cache 的页面映射。

---

## 1. FAMetadata - 元数据结构

```python
@dataclass
class FAMetadata(BaseAttnMetadata):
    cu_seqlens_k: torch.Tensor   # [padded_bs+1] KV 累积长度
    cu_seqlens_q: torch.Tensor   # [padded_bs+1] Q 累积长度
    cache_seqlens: torch.Tensor  # [padded_bs] 每个请求的 KV 总长度
    max_seqlen_k: int            # 最大 KV 序列长度
    max_seqlen_q: int            # 最大 Q 序列长度
    page_table: torch.Tensor     # [padded_bs, max_seqlen_k] 子页表
```

### cu_seqlens 的含义

`cu_seqlens` = cumulative sequence lengths，是 FlashAttention 的核心输入格式。

```
请求: [req0(len=3), req1(len=5), req2(len=2)]
cu_seqlens_k = [0, 3, 8, 10]   # cumsum
cu_seqlens_q:
  - decode 模式: [0, 1, 2, 3]  # 每个请求只查询1个token
  - prefill 无缓存: = cu_seqlens_k
  - prefill 有缓存: 累积 extend_len
```

---

## 2. prepare_metadata() - 元数据准备

```python
def prepare_metadata(self, batch: Batch) -> None:
    reqs = batch.padded_reqs
    seqlens_q = [req.extend_len for req in reqs]     # 本次要计算的 Q 长度
    seqlens_k = [req.device_len for req in reqs]     # 总 KV 长度
    cached_lens = [req.cached_len for req in reqs]   # 已缓存长度

    # cache_seqlens: 每个请求的完整 KV 长度
    cache_seqlens = torch.tensor(seqlens_k, ...).to(device)

    # cu_seqlens_k: KV 累积长度
    cu_seqlens_k = torch.tensor([0] + seqlens_k, ...).cumsum_(dim=0).to(device)

    # cu_seqlens_q 的三种情况:
    if max_seqlen_q == 1:                    # Decode: 每个请求 Q 长度=1
        cu_seqlens_q = arange(0, padded_size + 1)
    elif all(l == 0 for l in cached_lens):   # Prefill 无缓存命中: Q=K
        cu_seqlens_q = cu_seqlens_k
    else:                                     # Prefill 有部分缓存
        cu_seqlens_q = cumsum([0] + seqlens_q)

    # 子页表: 从全局 page_table 中提取每个请求的行
    new_page_table = torch.stack([
        page_table[req.table_idx, :max_seqlen_k] for req in reqs
    ])
```

关键点：FA 需要一个 **2D 子页表** `[padded_bs, max_seqlen_k]`，从全局 page_table 中按请求截取并堆叠。

---

## 3. forward() - 前向计算

```python
def forward(self, q, k, v, layer_id, batch):
    metadata = batch.attn_metadata
    # 1. 存储新的 K, V 到缓存
    self.kvcache.store_kv(k, v, batch.out_loc, layer_id)
    # 2. 调用 FlashAttention 核心
    return _fa_sgl_impl(
        q=q,
        k_cache=self.kvcache.k_cache(layer_id),
        v_cache=self.kvcache.v_cache(layer_id),
        page_table=metadata.page_table,
        cache_seqlens=metadata.cache_seqlens,
        cu_seqlens_q=metadata.cu_seqlens_q,
        cu_seqlens_k_new=metadata.cu_seqlens_k,
        max_seqlen_q=metadata.max_seqlen_q,
        softmax_scale=self.scale,        # 1/sqrt(head_dim)
    )
```

### _fa_sgl_impl 参数说明

```python
flash_attn_with_kvcache(
    q=q,                      # 查询张量
    k_cache=k_cache,          # 整层的 K 缓存
    v_cache=v_cache,          # 整层的 V 缓存
    page_table=page_table,    # 页面映射表
    cache_seqlens=...,        # 每个请求的 KV 总长
    cu_seqlens_q=...,         # Q 累积长度
    cu_seqlens_k_new=...,     # K 累积长度
    max_seqlen_q=...,         # 最大 Q 长度
    softmax_scale=...,        # 注意力缩放因子
    causal=True,              # 因果遮罩
    ver=3,                    # 使用 FA3 版本
)
```

---

## 4. CUDA 图支持

### 初始化 (`init_capture_graph`)

```python
def init_capture_graph(self, max_seq_len, bs_list):
    capture = FACaptureData.create(max_bs, max_seq_len, device)
    # FACaptureData 继承 BaseCaptureData，包含固定大小的缓冲区:
    # - seq_lens: [max_bs]
    # - cu_seqlens_k/q: [max_bs+1]
    # - page_table: [max_bs, max_seq_len]
```

### 捕获准备 (`prepare_for_capture`)

```python
def prepare_for_capture(self, batch):
    # 用固定缓冲区的切片构建 FAMetadata
    metadata = FAMetadata(
        cu_seqlens_k=capture.cu_seqlens_k[:bs+1],
        cu_seqlens_q=capture.cu_seqlens_q[:bs+1],
        cache_seqlens=capture.seq_lens[:bs],
        max_seqlen_k=capture.page_table.size(1),  # 固定为 max_seq_len
        max_seqlen_q=1,                            # decode 恒为 1
        page_table=capture.page_table[:bs, :],
    )
```

### 回放准备 (`prepare_for_replay`)

```python
def prepare_for_replay(self, batch):
    # 将实际数据拷入固定缓冲区（地址不变，值更新）
    self.capture.cu_seqlens_k[:bs+1].copy_(metadata.cu_seqlens_k)
    self.capture.seq_lens[:bs].copy_(metadata.cache_seqlens)
    self.capture.page_table[:bs, :max_k].copy_(metadata.page_table)
```

CUDA 图要求所有输入张量的地址固定。FA 的做法是：
1. 捕获时使用 `capture` 缓冲区的**切片**（地址固定）
2. 回放前将实际数据 `copy_` 到缓冲区中

---

## 关键设计总结

| 设计 | 说明 |
|------|------|
| 2D Page Table | `[bs, max_seq_len]`，每个请求一行，直观但需要额外 `torch.stack` |
| FA3 | 使用 `ver=3` 调用 Hopper 优化的 FA3 内核 |
| cu_seqlens 三路径 | Decode/Prefill无缓存/Prefill有缓存，分别构建 Q 累积长度 |
| 固定缓冲区 | CUDA 图使用 `BaseCaptureData` 的固定地址切片 |

# 3.1 注意力后端对比

> 源文件：`python/minisgl/attention/__init__.py`, `attention/base.py`

## 概览

mini-sGLang 支持两种注意力后端和一种混合模式：

| 后端 | 标识 | 底层库 | 特点 |
|------|------|--------|------|
| FlashAttention | `fa` | `sgl_kernel.flash_attn` | Prefill 性能好 (Hopper+) |
| FlashInfer | `fi` | `flashinfer` | Decode 性能好，支持 Tensor Core 加速 |
| Hybrid | `fa,fi` | 两者结合 | Prefill 用 FA，Decode 用 FI |

---

## 1. 自动选择策略

```python
def resolve_auto_backend() -> str:
    if is_sm100_supported():      # Blackwell (SM100)
        return "fi"
    elif is_sm90_supported():     # Hopper (SM90)
        return "fa,fi"            # Hybrid: FA prefill + FI decode
    else:                         # Pre-Hopper
        return "fi"
```

- **Blackwell (SM100)**：仅用 FlashInfer
- **Hopper (SM90)**：Hybrid 模式，FA 处理 prefill，FI 处理 decode
- **Pre-Hopper**：仅用 FlashInfer

---

## 2. BaseAttnBackend 接口

```python
class BaseAttnBackend(ABC):
    def forward(self, q, k, v, layer_id, batch) -> Tensor     # 注意力计算
    def prepare_metadata(self, batch) -> None                   # 准备元数据
    def init_capture_graph(self, max_seq_len, bs_list) -> None  # 初始化CUDA图捕获
    def prepare_for_capture(self, batch) -> None                # 单个bs的捕获准备
    def prepare_for_replay(self, batch) -> None                 # 回放前准备
```

### forward 调用时机

```
模型层 RopeAttn.forward()
    │
    ├─ 计算 Q, K, V
    ├─ 应用 RoPE
    │
    ▼
attn_backend.forward(q, k, v, layer_id, batch)
    │
    ├─ store_kv(k, v, out_loc, layer_id)   # 写入 KV cache
    └─ 计算 attention output               # 从 KV cache 读取
```

---

## 3. HybridBackend - 混合模式

```python
class HybridBackend(BaseAttnBackend):
    def __init__(self, prefill_backend, decode_backend):
        self.prefill_backend = prefill_backend
        self.decode_backend = decode_backend

    def forward(self, q, k, v, layer_id, batch):
        backend = self.prefill_backend if batch.is_prefill else self.decode_backend
        return backend.forward(q, k, v, layer_id, batch)
```

HybridBackend 根据 `batch.phase` 自动路由到对应后端。CUDA 图相关操作只委托给 decode_backend（因为只有 decode 使用 CUDA 图）。

---

## 4. BaseAttnMetadata 与 CUDA 图

```python
@dataclass
class BaseAttnMetadata(ABC):
    def get_last_indices(self, bs: int) -> torch.Tensor: ...
```

`get_last_indices` 返回每个请求最后一个 token 的索引，用于从 attention 输出中提取 logits。

### BaseCaptureData

```python
@dataclass
class BaseCaptureData:
    seq_lens: torch.Tensor       # [max_bs]
    positions: torch.Tensor      # [max_bs]
    cu_seqlens_k: torch.Tensor   # [max_bs+1] cumulative KV lengths
    cu_seqlens_q: torch.Tensor   # [max_bs+1] cumulative Q lengths
    page_table: torch.Tensor     # [max_bs, max_seq_len]
```

这是 CUDA 图捕获时使用的固定缓冲区。回放时将实际值拷入这些缓冲区，保持图的输入地址不变。

---

## 5. 注册表模式

```python
SUPPORTED_ATTENTION_BACKENDS = Registry[BackendCreator]("Attention Backend")

@SUPPORTED_ATTENTION_BACKENDS.register("fi")
def create_fi_backend(config, kvcache, page_table):
    from .fi import FlashInferBackend
    return FlashInferBackend(config, kvcache, page_table)

@SUPPORTED_ATTENTION_BACKENDS.register("fa")
def create_fa_backend(config, kvcache, page_table):
    from .fa import FlashAttentionBackend
    return FlashAttentionBackend(config, kvcache, page_table)
```

延迟导入 + 注册表，只在实际使用时加载对应后端。

---

## 关键对比总结

| 维度 | FlashAttention (fa) | FlashInfer (fi) |
|------|-------------------|----------------|
| 底层依赖 | `sgl_kernel` | `flashinfer` |
| KV cache 索引 | 2D page_table `[bs, max_seq_len]` | 1D ragged indices (展平) |
| wrapper 模式 | 无 wrapper，直接调用 | BatchPrefill/DecodeWrapper + plan() |
| CUDA 图 | 直接拷贝元数据 | CUDAGraphBatchDecodeWrapper |
| Tensor Core | N/A | GQA >= 4 时启用 |
| metadata 初始化 | 立即生效 | 延迟初始化 (`_initialize_metadata_once`) |
| Hopper 优势 | FA3 prefill | decode 性能 |
